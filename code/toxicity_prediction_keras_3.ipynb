{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "toxicity_prediction_keras.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "epiTpMk96vR0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPu4Nt9q6vR3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding, Bidirectional\n",
        "from tensorflow.keras.layers import Input, Flatten, Activation, RepeatVector\n",
        "from tensorflow.keras.layers import Permute, multiply, Lambda, Dropout\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.callbacks import LambdaCallback\n",
        "from keras import backend as K\n",
        "\n",
        "from tensorflow.keras import optimizers\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import time\n",
        "import math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6Rg7ctX6vR6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Helper function to plot results\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)\n",
        "    \n",
        "# Precision = TP / (TP+FP).\n",
        "def precision(decoded_vals, target_vals):\n",
        "    TP = 0\n",
        "    FP = 0\n",
        "    for i in range(len(decoded_vals)):\n",
        "        TP += int(decoded_vals[i] == 0 and target_vals[i] == 0)\n",
        "        FP += int(decoded_vals[i] == 0 and target_vals[i] == 1)\n",
        "\n",
        "    return 1.0 * TP / (TP + FP)\n",
        "\n",
        "# Recall = TP / (TP+FN).\n",
        "def recall(decoded_vals, target_vals):\n",
        "    TP = 0\n",
        "    FN = 0\n",
        "    for i in range(len(decoded_vals)):\n",
        "        TP += int(decoded_vals[i] == 0 and target_vals[i] == 0)\n",
        "        FN += int(decoded_vals[i] == 1 and target_vals[i] == 0)\n",
        "\n",
        "    return 1.0 * TP / (TP + FN)\n",
        "\n",
        "def find_balanced_accuracy(decoded_vals, target_vals):\n",
        "    P = 0\n",
        "    TP = 0\n",
        "    N = 0\n",
        "    TN = 0\n",
        "    for i in range(len(decoded_vals)):\n",
        "        P += int(target_vals[i] == 0)\n",
        "        TP += int(decoded_vals[i] == 0 and target_vals[i] == 0)\n",
        "        N += int(target_vals[i] == 1)\n",
        "        TN += int(decoded_vals[i] == 1 and target_vals[i] == 1)\n",
        "    return (TP/P + TN/N)/2.0\n",
        "\n",
        "def evaluate_metrics(model, X_test, y_test):\n",
        "    predictions = model.predict(X_test)\n",
        "    rounded = [round(x[0]) for x in predictions]\n",
        "    # predictions = model.predict_classes(X_test)\n",
        "    print(\"Precision: \" + str(precision(rounded, y_test)))\n",
        "    print(\"Recall: \" + str(recall(rounded, y_test)))\n",
        "    print(\"Balanced Accuracy: \" + str(find_balanced_accuracy(rounded, y_test)))    \n",
        "    return predictions, rounded"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "yL_OtAIA6vR8",
        "colab_type": "code",
        "outputId": "b316301d-567d-4614-f366-86d140fbc6d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Load data.\n",
        "drugbank_smile_data_train_url = \"https://raw.githubusercontent.com/vli1721/cs281-project/master/drugbank_smile_data_filtered_train.csv?token=AHRDF6OBWOFKKK4AKOCY2N257WWVE\"\n",
        "drugbank_smile_data_test_url = \"https://raw.githubusercontent.com/vli1721/cs281-project/master/drugbank_smile_data_filtered_test.csv?token=AHRDF6NISTEXL5L2SDWX4XS57WWOY\"\n",
        "drugbank_smile_data_unlabeled_url = \"https://raw.githubusercontent.com/vli1721/cs281-project/master/drugbank_smile_data_unlabeled.csv?token=AHRDF6J7N3K6FJZTVVSB7DC57WWRQ\"\n",
        "\n",
        "smile_data_train = pd.read_csv(drugbank_smile_data_train_url)\n",
        "smile_data_test = pd.read_csv(drugbank_smile_data_test_url)\n",
        "smile_data_unlabeled = pd.read_csv(drugbank_smile_data_unlabeled_url)\n",
        "\n",
        "max_len_train = max(list(map(len, smile_data_train[\"smile\"])))\n",
        "max_len_test = max(list(map(len, smile_data_test[\"smile\"])))\n",
        "max_len_unlabeled = max(list(map(len, smile_data_unlabeled[\"smile\"])))\n",
        "max_len = max(max_len_train, max_len_test, max_len_unlabeled)\n",
        "\n",
        "print(\"Max Length Train: \" + str(max_len_train))\n",
        "print(\"Max Length Test: \" + str(max_len_test))\n",
        "print(\"Max Length Unlabeled: \" + str(max_len_unlabeled))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max Length Train: 1526\n",
            "Max Length Test: 351\n",
            "Max Length Unlabeled: 1695\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJKdCkUX6vSA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Constants\n",
        "NUM_CHARS = 128\n",
        "INPUT_SIZE = max_len\n",
        "MAX_LENGTH = max_len\n",
        "HIDDEN_SIZE = 100\n",
        "EMBEDDING_SIZE = 64\n",
        "NUM_LAYERS = 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVrMTMOK6vSC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# X-values are SMILE strings, encoded as tensors of ASCII values.\n",
        "# Y-values are toxicity ratings (1 indicates toxic, 0 indicates non-toxic).\n",
        "X_train = []\n",
        "y_train = []\n",
        "X_test = []\n",
        "y_test = []\n",
        "\n",
        "for index, row in smile_data_train.iterrows():\n",
        "    X_train.append(list(map(ord, row[\"smile\"])))\n",
        "    y_train.append(row[\"toxicity\"])\n",
        "for index, row in smile_data_test.iterrows():\n",
        "    X_test.append(list(map(ord, row[\"smile\"])))\n",
        "    y_test.append(row[\"toxicity\"])\n",
        "\n",
        "X_train = sequence.pad_sequences(np.array(X_train), maxlen=MAX_LENGTH)\n",
        "y_train = np.array(y_train)\n",
        "X_test = sequence.pad_sequences(np.array(X_test), maxlen=MAX_LENGTH)\n",
        "y_test = np.array(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2sFU5ilIAhL",
        "colab_type": "code",
        "outputId": "e50bdb41-e84c-4320-8117-e6babf445e45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(\"Proportion of non-toxic drugs in train set: \" + str((len(y_train) - sum(y_train))/len(y_train)))\n",
        "print(\"Proportion of non-toxic drugs in test set: \" + str((len(y_test) - sum(y_test))/len(y_test)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Proportion of non-toxic drugs in train set: 0.9163498098859315\n",
            "Proportion of non-toxic drugs in test set: 0.920303605313093\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUZUn1WW6vSF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate train main-validation split (80-20).\n",
        "X_train_main, X_train_val, y_train_main, y_train_val = train_test_split(np.array(X_train), np.array(y_train), train_size=0.8, test_size=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XC0U46CLuY-m",
        "colab_type": "code",
        "outputId": "5eb106c8-313b-4557-e9c3-4ba1170e8162",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Stratified Sampling: Set class weights for imbalanced classes (https://datascience.stackexchange.com/questions/13490/how-to-set-class-weights-for-imbalanced-classes-in-keras).\n",
        "class_weights = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\n",
        "class_weights_dict = dict(enumerate(class_weights))\n",
        "class_weights_dict"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 0.5456431535269709, 1: 5.9772727272727275}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "4bb46bf4-4bb7-4f85-95c6-27e81c2124a2",
        "id": "ts0oS7JzAiUP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 786
        }
      },
      "source": [
        "# LSTM without attention. (https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/)\n",
        "lstm_no_attention = Sequential()\n",
        "lstm_no_attention.add(Embedding(NUM_CHARS, EMBEDDING_SIZE, input_length=MAX_LENGTH))\n",
        "lstm_no_attention.add(LSTM(HIDDEN_SIZE))\n",
        "lstm_no_attention.add(Dense(1, activation='sigmoid'))\n",
        "lstm_no_attention.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(lstm_no_attention.summary())\n",
        "\n",
        "print_weights_lstm_no_attention = LambdaCallback(on_epoch_end=lambda epoch, logs: print(lstm_no_attention.layers[2].get_weights()))\n",
        "\n",
        "lstm_no_attention.fit(X_train_main, y_train_main,\n",
        "                      validation_data=(X_train_val, y_train_val),\n",
        "                      epochs=3, batch_size=1024, class_weight=class_weights_dict,\n",
        "                      callbacks=[print_weights_lstm_no_attention])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 1695, 64)          8192      \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 100)               66000     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 74,293\n",
            "Trainable params: 74,293\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 1683 samples, validate on 421 samples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-067525dced5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m                       \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                       \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weights_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                       callbacks=[print_weights_lstm_no_attention])\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0;31m# Setup work for each epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0mepoch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mreset_metrics\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    912\u001b[0m     \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_training_eval_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m       \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m     \u001b[0;31m# Reset metrics on all the distributed (cloned) models.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/metrics.py\u001b[0m in \u001b[0;36mreset_states\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0mwhen\u001b[0m \u001b[0ma\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mevaluated\u001b[0m \u001b[0mduring\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \"\"\"\n\u001b[0;32m--> 210\u001b[0;31m     \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36mbatch_set_value\u001b[0;34m(tuples)\u001b[0m\n\u001b[1;32m   3257\u001b[0m           \u001b[0massign_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3258\u001b[0m           \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0massign_placeholder\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3259\u001b[0;31m         \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m(op_input_list)\u001b[0m\n\u001b[1;32m    481\u001b[0m       \u001b[0mA\u001b[0m \u001b[0mTensorFlow\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m   \"\"\"\n\u001b[0;32m--> 483\u001b[0;31m   \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_input_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_MANUAL_VAR_INIT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m_get_session\u001b[0;34m(op_input_list)\u001b[0m\n\u001b[1;32m    453\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m         _SESSION.session = session_module.Session(\n\u001b[0;32m--> 455\u001b[0;31m             config=get_default_session_config())\n\u001b[0m\u001b[1;32m    456\u001b[0m     \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_SESSION\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, target, graph, config)\u001b[0m\n\u001b[1;32m   1583\u001b[0m           \u001b[0mprotocol\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mconfiguration\u001b[0m \u001b[0moptions\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1584\u001b[0m     \"\"\"\n\u001b[0;32m-> 1585\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1586\u001b[0m     \u001b[0;31m# NOTE(mrry): Create these on first `__enter__` to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1587\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_graph_context_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, target, graph, config)\u001b[0m\n\u001b[1;32m    697\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m       \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_NewSessionRef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m       \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLZD2Gte-rWF",
        "colab_type": "code",
        "outputId": "00996dc0-848e-4b06-b8d2-d7ab360ece09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "predictions_lstm_no_attention, rounded_lstm_no_attention = evaluate_metrics(lstm_no_attention, X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision: 0.9605911330049262\n",
            "Recall: 0.4020618556701031\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0NyXdfk7orF",
        "colab_type": "code",
        "outputId": "4a9e86e7-c1a4-44c2-86cb-b9cf37fb3a6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "predictions_lstm_no_attention"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.5101005 ],\n",
              "       [0.50500476],\n",
              "       [0.4897222 ],\n",
              "       [0.49900225],\n",
              "       [0.48974425],\n",
              "       [0.5101749 ],\n",
              "       [0.48697817],\n",
              "       [0.48942205],\n",
              "       [0.50651914],\n",
              "       [0.5128597 ],\n",
              "       [0.5117712 ],\n",
              "       [0.5082884 ],\n",
              "       [0.51310635],\n",
              "       [0.51035637],\n",
              "       [0.5092134 ],\n",
              "       [0.48816264],\n",
              "       [0.5129068 ],\n",
              "       [0.49480304],\n",
              "       [0.4857381 ],\n",
              "       [0.4995061 ],\n",
              "       [0.5073859 ],\n",
              "       [0.49647665],\n",
              "       [0.50858593],\n",
              "       [0.48736432],\n",
              "       [0.5119317 ],\n",
              "       [0.50619984],\n",
              "       [0.49524066],\n",
              "       [0.4864872 ],\n",
              "       [0.51082534],\n",
              "       [0.48783225],\n",
              "       [0.4912184 ],\n",
              "       [0.49368536],\n",
              "       [0.512452  ],\n",
              "       [0.5059712 ],\n",
              "       [0.5103349 ],\n",
              "       [0.5077276 ],\n",
              "       [0.50681204],\n",
              "       [0.5097348 ],\n",
              "       [0.5128153 ],\n",
              "       [0.5125508 ],\n",
              "       [0.50995976],\n",
              "       [0.5031809 ],\n",
              "       [0.4883445 ],\n",
              "       [0.509873  ],\n",
              "       [0.50832003],\n",
              "       [0.512671  ],\n",
              "       [0.513168  ],\n",
              "       [0.4877651 ],\n",
              "       [0.4987    ],\n",
              "       [0.48890996],\n",
              "       [0.48934987],\n",
              "       [0.5046438 ],\n",
              "       [0.48748487],\n",
              "       [0.50992775],\n",
              "       [0.49091592],\n",
              "       [0.49219   ],\n",
              "       [0.49440083],\n",
              "       [0.5101077 ],\n",
              "       [0.50436944],\n",
              "       [0.511136  ],\n",
              "       [0.49405998],\n",
              "       [0.510516  ],\n",
              "       [0.48665962],\n",
              "       [0.48722482],\n",
              "       [0.5139388 ],\n",
              "       [0.50437564],\n",
              "       [0.51286197],\n",
              "       [0.48971644],\n",
              "       [0.4938389 ],\n",
              "       [0.50883377],\n",
              "       [0.5077277 ],\n",
              "       [0.5092434 ],\n",
              "       [0.50985694],\n",
              "       [0.48765245],\n",
              "       [0.5128045 ],\n",
              "       [0.5048013 ],\n",
              "       [0.51109874],\n",
              "       [0.5118298 ],\n",
              "       [0.49770215],\n",
              "       [0.50342155],\n",
              "       [0.51279706],\n",
              "       [0.48716998],\n",
              "       [0.51076025],\n",
              "       [0.51045537],\n",
              "       [0.48956692],\n",
              "       [0.48723152],\n",
              "       [0.51263165],\n",
              "       [0.49523994],\n",
              "       [0.49770162],\n",
              "       [0.49312806],\n",
              "       [0.4929961 ],\n",
              "       [0.48720568],\n",
              "       [0.48672974],\n",
              "       [0.5079233 ],\n",
              "       [0.51300675],\n",
              "       [0.5029204 ],\n",
              "       [0.48699692],\n",
              "       [0.5010309 ],\n",
              "       [0.5109363 ],\n",
              "       [0.50469387],\n",
              "       [0.50386286],\n",
              "       [0.4872052 ],\n",
              "       [0.5061935 ],\n",
              "       [0.4963582 ],\n",
              "       [0.5111644 ],\n",
              "       [0.5125874 ],\n",
              "       [0.50952685],\n",
              "       [0.49848178],\n",
              "       [0.51256126],\n",
              "       [0.50065744],\n",
              "       [0.49710852],\n",
              "       [0.49130514],\n",
              "       [0.5127579 ],\n",
              "       [0.48862788],\n",
              "       [0.5128849 ],\n",
              "       [0.49001187],\n",
              "       [0.5028616 ],\n",
              "       [0.4929805 ],\n",
              "       [0.49783185],\n",
              "       [0.5115008 ],\n",
              "       [0.511879  ],\n",
              "       [0.5128588 ],\n",
              "       [0.48680773],\n",
              "       [0.5021114 ],\n",
              "       [0.48945633],\n",
              "       [0.5014269 ],\n",
              "       [0.48725283],\n",
              "       [0.50748247],\n",
              "       [0.51280725],\n",
              "       [0.49777138],\n",
              "       [0.48671746],\n",
              "       [0.49656144],\n",
              "       [0.49748245],\n",
              "       [0.5129092 ],\n",
              "       [0.5045646 ],\n",
              "       [0.49767715],\n",
              "       [0.5088995 ],\n",
              "       [0.49143416],\n",
              "       [0.51286197],\n",
              "       [0.50289106],\n",
              "       [0.49604565],\n",
              "       [0.48717695],\n",
              "       [0.49713933],\n",
              "       [0.5127667 ],\n",
              "       [0.510929  ],\n",
              "       [0.5085633 ],\n",
              "       [0.50464296],\n",
              "       [0.4957571 ],\n",
              "       [0.5085635 ],\n",
              "       [0.5062602 ],\n",
              "       [0.50250196],\n",
              "       [0.50367934],\n",
              "       [0.5128407 ],\n",
              "       [0.497478  ],\n",
              "       [0.5129547 ],\n",
              "       [0.5112972 ],\n",
              "       [0.51015455],\n",
              "       [0.5037919 ],\n",
              "       [0.5105616 ],\n",
              "       [0.5118005 ],\n",
              "       [0.51161516],\n",
              "       [0.49575937],\n",
              "       [0.49300778],\n",
              "       [0.50047994],\n",
              "       [0.5104695 ],\n",
              "       [0.5061599 ],\n",
              "       [0.48774388],\n",
              "       [0.50393903],\n",
              "       [0.48680556],\n",
              "       [0.50486   ],\n",
              "       [0.48875353],\n",
              "       [0.5084285 ],\n",
              "       [0.48907164],\n",
              "       [0.50802624],\n",
              "       [0.5056119 ],\n",
              "       [0.48748615],\n",
              "       [0.51298887],\n",
              "       [0.51273763],\n",
              "       [0.515287  ],\n",
              "       [0.5100018 ],\n",
              "       [0.48876452],\n",
              "       [0.48723745],\n",
              "       [0.5128066 ],\n",
              "       [0.5113791 ],\n",
              "       [0.51215833],\n",
              "       [0.5067586 ],\n",
              "       [0.51272345],\n",
              "       [0.50231504],\n",
              "       [0.50726515],\n",
              "       [0.48910454],\n",
              "       [0.49664566],\n",
              "       [0.48680496],\n",
              "       [0.50857747],\n",
              "       [0.50423735],\n",
              "       [0.5101709 ],\n",
              "       [0.5034156 ],\n",
              "       [0.51280606],\n",
              "       [0.4951817 ],\n",
              "       [0.4935242 ],\n",
              "       [0.509716  ],\n",
              "       [0.48633695],\n",
              "       [0.49857438],\n",
              "       [0.5091508 ],\n",
              "       [0.49421057],\n",
              "       [0.48915365],\n",
              "       [0.4961109 ],\n",
              "       [0.48433262],\n",
              "       [0.51009524],\n",
              "       [0.4877724 ],\n",
              "       [0.50514174],\n",
              "       [0.48901775],\n",
              "       [0.49521443],\n",
              "       [0.51280814],\n",
              "       [0.5107406 ],\n",
              "       [0.5037032 ],\n",
              "       [0.50998783],\n",
              "       [0.5110215 ],\n",
              "       [0.515341  ],\n",
              "       [0.5127189 ],\n",
              "       [0.5002457 ],\n",
              "       [0.4979149 ],\n",
              "       [0.50042987],\n",
              "       [0.49012208],\n",
              "       [0.50677055],\n",
              "       [0.512727  ],\n",
              "       [0.51281923],\n",
              "       [0.5097744 ],\n",
              "       [0.48781908],\n",
              "       [0.48765796],\n",
              "       [0.5017393 ],\n",
              "       [0.51278955],\n",
              "       [0.5070423 ],\n",
              "       [0.50867563],\n",
              "       [0.50608975],\n",
              "       [0.5065821 ],\n",
              "       [0.50641966],\n",
              "       [0.50476843],\n",
              "       [0.51286197],\n",
              "       [0.48776987],\n",
              "       [0.48944464],\n",
              "       [0.48752096],\n",
              "       [0.48916635],\n",
              "       [0.5153408 ],\n",
              "       [0.4996848 ],\n",
              "       [0.5038884 ],\n",
              "       [0.48577535],\n",
              "       [0.49750426],\n",
              "       [0.4999886 ],\n",
              "       [0.5000216 ],\n",
              "       [0.5128391 ],\n",
              "       [0.51051897],\n",
              "       [0.4877802 ],\n",
              "       [0.48944816],\n",
              "       [0.48934987],\n",
              "       [0.48926184],\n",
              "       [0.50000167],\n",
              "       [0.48742852],\n",
              "       [0.50452393],\n",
              "       [0.5127346 ],\n",
              "       [0.5055155 ],\n",
              "       [0.5047392 ],\n",
              "       [0.4927812 ],\n",
              "       [0.49801847],\n",
              "       [0.5093494 ],\n",
              "       [0.5128045 ],\n",
              "       [0.50444484],\n",
              "       [0.5046774 ],\n",
              "       [0.5125136 ],\n",
              "       [0.5107549 ],\n",
              "       [0.50865936],\n",
              "       [0.49854383],\n",
              "       [0.4946252 ],\n",
              "       [0.50973976],\n",
              "       [0.51200277],\n",
              "       [0.5127791 ],\n",
              "       [0.5093631 ],\n",
              "       [0.512738  ],\n",
              "       [0.5127306 ],\n",
              "       [0.48850086],\n",
              "       [0.51282406],\n",
              "       [0.4938955 ],\n",
              "       [0.5086235 ],\n",
              "       [0.49593776],\n",
              "       [0.49778482],\n",
              "       [0.49506015],\n",
              "       [0.5087513 ],\n",
              "       [0.51248896],\n",
              "       [0.5047349 ],\n",
              "       [0.4889332 ],\n",
              "       [0.48815244],\n",
              "       [0.5002564 ],\n",
              "       [0.49499077],\n",
              "       [0.49527782],\n",
              "       [0.50952375],\n",
              "       [0.48749042],\n",
              "       [0.4998824 ],\n",
              "       [0.5079679 ],\n",
              "       [0.49084988],\n",
              "       [0.5080812 ],\n",
              "       [0.50178546],\n",
              "       [0.5096145 ],\n",
              "       [0.50856704],\n",
              "       [0.5062193 ],\n",
              "       [0.51054114],\n",
              "       [0.5101606 ],\n",
              "       [0.5078448 ],\n",
              "       [0.49266464],\n",
              "       [0.51321995],\n",
              "       [0.4947439 ],\n",
              "       [0.50566703],\n",
              "       [0.50376844],\n",
              "       [0.48701844],\n",
              "       [0.5128054 ],\n",
              "       [0.49755716],\n",
              "       [0.5087156 ],\n",
              "       [0.51030064],\n",
              "       [0.51283354],\n",
              "       [0.5016768 ],\n",
              "       [0.4971872 ],\n",
              "       [0.4867364 ],\n",
              "       [0.51304036],\n",
              "       [0.5084783 ],\n",
              "       [0.49329877],\n",
              "       [0.48765853],\n",
              "       [0.50502235],\n",
              "       [0.50447106],\n",
              "       [0.50444233],\n",
              "       [0.5098017 ],\n",
              "       [0.5062681 ],\n",
              "       [0.5077655 ],\n",
              "       [0.51270574],\n",
              "       [0.51285666],\n",
              "       [0.48918772],\n",
              "       [0.5002345 ],\n",
              "       [0.5067105 ],\n",
              "       [0.49366683],\n",
              "       [0.5042456 ],\n",
              "       [0.519614  ],\n",
              "       [0.5126035 ],\n",
              "       [0.48767132],\n",
              "       [0.48683113],\n",
              "       [0.4877083 ],\n",
              "       [0.48701182],\n",
              "       [0.50857216],\n",
              "       [0.51273763],\n",
              "       [0.50712484],\n",
              "       [0.49371395],\n",
              "       [0.5002521 ],\n",
              "       [0.49298778],\n",
              "       [0.50368154],\n",
              "       [0.4931644 ],\n",
              "       [0.5098657 ],\n",
              "       [0.49418476],\n",
              "       [0.5094242 ],\n",
              "       [0.49138537],\n",
              "       [0.5072695 ],\n",
              "       [0.48653927],\n",
              "       [0.48903492],\n",
              "       [0.49440193],\n",
              "       [0.50733703],\n",
              "       [0.5128177 ],\n",
              "       [0.48743308],\n",
              "       [0.501981  ],\n",
              "       [0.5154062 ],\n",
              "       [0.5090868 ],\n",
              "       [0.5128694 ],\n",
              "       [0.5124824 ],\n",
              "       [0.5070719 ],\n",
              "       [0.5146379 ],\n",
              "       [0.5104279 ],\n",
              "       [0.48673552],\n",
              "       [0.50988275],\n",
              "       [0.5083634 ],\n",
              "       [0.50989056],\n",
              "       [0.5095991 ],\n",
              "       [0.49703142],\n",
              "       [0.49539128],\n",
              "       [0.5120318 ],\n",
              "       [0.5128618 ],\n",
              "       [0.50695735],\n",
              "       [0.5127644 ],\n",
              "       [0.50007606],\n",
              "       [0.51292133],\n",
              "       [0.48582   ],\n",
              "       [0.50560373],\n",
              "       [0.487146  ],\n",
              "       [0.50574994],\n",
              "       [0.51163447],\n",
              "       [0.49440193],\n",
              "       [0.49662974],\n",
              "       [0.5033074 ],\n",
              "       [0.48808238],\n",
              "       [0.49330005],\n",
              "       [0.49477464],\n",
              "       [0.5107621 ],\n",
              "       [0.5067415 ],\n",
              "       [0.50860083],\n",
              "       [0.48671222],\n",
              "       [0.493973  ],\n",
              "       [0.5128408 ],\n",
              "       [0.48774424],\n",
              "       [0.5103077 ],\n",
              "       [0.5110017 ],\n",
              "       [0.50485826],\n",
              "       [0.51053387],\n",
              "       [0.51282287],\n",
              "       [0.48729908],\n",
              "       [0.50343615],\n",
              "       [0.48668534],\n",
              "       [0.48775768],\n",
              "       [0.5127999 ],\n",
              "       [0.4890384 ],\n",
              "       [0.4961656 ],\n",
              "       [0.50549674],\n",
              "       [0.4931563 ],\n",
              "       [0.50135916],\n",
              "       [0.49084252],\n",
              "       [0.5082691 ],\n",
              "       [0.50227475],\n",
              "       [0.5128768 ],\n",
              "       [0.504198  ],\n",
              "       [0.51278806],\n",
              "       [0.5130665 ],\n",
              "       [0.4892878 ],\n",
              "       [0.5129756 ],\n",
              "       [0.49711105],\n",
              "       [0.49291262],\n",
              "       [0.51286167],\n",
              "       [0.5014135 ],\n",
              "       [0.4996671 ],\n",
              "       [0.50231606],\n",
              "       [0.51295066],\n",
              "       [0.4897678 ],\n",
              "       [0.51284176],\n",
              "       [0.51278555],\n",
              "       [0.50864303],\n",
              "       [0.5104221 ],\n",
              "       [0.5090512 ],\n",
              "       [0.51269996],\n",
              "       [0.5086771 ],\n",
              "       [0.48753145],\n",
              "       [0.50802183],\n",
              "       [0.48871258],\n",
              "       [0.5128105 ],\n",
              "       [0.5105191 ],\n",
              "       [0.51279926],\n",
              "       [0.50991154],\n",
              "       [0.501547  ],\n",
              "       [0.5093633 ],\n",
              "       [0.49873292],\n",
              "       [0.49888387],\n",
              "       [0.48653927],\n",
              "       [0.50372213],\n",
              "       [0.50488585],\n",
              "       [0.5127422 ],\n",
              "       [0.49744517],\n",
              "       [0.5027642 ],\n",
              "       [0.48912644],\n",
              "       [0.48721865],\n",
              "       [0.50334644],\n",
              "       [0.5078663 ],\n",
              "       [0.50325423],\n",
              "       [0.4997089 ],\n",
              "       [0.48761863],\n",
              "       [0.4949852 ],\n",
              "       [0.50738883],\n",
              "       [0.4872895 ],\n",
              "       [0.48653924],\n",
              "       [0.4964863 ],\n",
              "       [0.48680055],\n",
              "       [0.48643228],\n",
              "       [0.5107295 ],\n",
              "       [0.4877711 ],\n",
              "       [0.5082857 ],\n",
              "       [0.51009506],\n",
              "       [0.48875117],\n",
              "       [0.48884538],\n",
              "       [0.51279926],\n",
              "       [0.50624734],\n",
              "       [0.50936586],\n",
              "       [0.5004142 ],\n",
              "       [0.50952864],\n",
              "       [0.50558656],\n",
              "       [0.5038118 ],\n",
              "       [0.5062166 ],\n",
              "       [0.50372225],\n",
              "       [0.512738  ],\n",
              "       [0.48880547],\n",
              "       [0.4977671 ],\n",
              "       [0.51016563],\n",
              "       [0.491559  ],\n",
              "       [0.49577796],\n",
              "       [0.5026447 ],\n",
              "       [0.48436213],\n",
              "       [0.49508274],\n",
              "       [0.5049099 ],\n",
              "       [0.51013976],\n",
              "       [0.50929666],\n",
              "       [0.4988139 ],\n",
              "       [0.5127191 ],\n",
              "       [0.5046357 ],\n",
              "       [0.5127992 ],\n",
              "       [0.4859026 ],\n",
              "       [0.5052419 ],\n",
              "       [0.50592077],\n",
              "       [0.5149915 ],\n",
              "       [0.509915  ],\n",
              "       [0.5033001 ],\n",
              "       [0.514891  ],\n",
              "       [0.5086139 ],\n",
              "       [0.51153564],\n",
              "       [0.50450087],\n",
              "       [0.5128956 ],\n",
              "       [0.5100266 ],\n",
              "       [0.49576372],\n",
              "       [0.5087229 ],\n",
              "       [0.5098465 ],\n",
              "       [0.4879538 ],\n",
              "       [0.51285803],\n",
              "       [0.50549996],\n",
              "       [0.48959336],\n",
              "       [0.485525  ],\n",
              "       [0.4872204 ],\n",
              "       [0.5081259 ],\n",
              "       [0.5077998 ],\n",
              "       [0.4980986 ],\n",
              "       [0.48653534]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 180
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSb3T-rrGkMG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # evaluate the keras model\n",
        "# _, accuracy = model.evaluate(X_test, y_test)\n",
        "# print('Accuracy: %.2f' % (accuracy*100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sp5arRkb-srl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://machinelearningmastery.com/binary-classification-tutorial-with-the-keras-deep-learning-library/\n",
        "# baseline model\n",
        "def create_baseline():\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(NUM_CHARS, EMBEDDING_SIZE, input_length=MAX_LENGTH))\n",
        "  model.add(LSTM(HIDDEN_SIZE))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64s3uLSdAgLR",
        "colab_type": "code",
        "outputId": "01b4218e-b712-4dfa-c5b2-a1d54c164b9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        }
      },
      "source": [
        "# evaluate model with standardized dataset\n",
        "estimator = KerasClassifier(build_fn=create_baseline, epochs=1, batch_size=64, verbose=1)\n",
        "kfold = StratifiedKFold(n_splits=3, shuffle=True)\n",
        "results = cross_val_score(estimator, X_train, y_train, cv=kfold)\n",
        "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Train on 1402 samples\n",
            "1402/1402 [==============================] - 50s 36ms/sample - loss: 0.4407 - acc: 0.8802\n",
            "702/702 [==============================] - 5s 7ms/sample - loss: 0.2978 - acc: 0.9145\n",
            "Train on 1403 samples\n",
            "1403/1403 [==============================] - 49s 35ms/sample - loss: 0.4426 - acc: 0.9016\n",
            "701/701 [==============================] - 5s 7ms/sample - loss: 0.2978 - acc: 0.9144\n",
            "Train on 1403 samples\n",
            "1403/1403 [==============================] - 49s 35ms/sample - loss: 0.4603 - acc: 0.8810\n",
            "701/701 [==============================] - 5s 7ms/sample - loss: 0.2973 - acc: 0.9144\n",
            "Baseline: 91.44% (0.01%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjQlqDcAAz4O",
        "colab_type": "code",
        "outputId": "023b8d69-fbcd-41ee-a0d8-9b1ccc16d457",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "estimator.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-687bf9adead9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, **kwargs)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \"\"\"\n\u001b[1;32m    240\u001b[0m     \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_sk_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'KerasClassifier' object has no attribute 'model'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ql82tvFIcqAE",
        "colab_type": "code",
        "outputId": "323c438d-504d-4d32-c378-3761388d1544",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        }
      },
      "source": [
        "# Bidirectional LSTM without Attention (https://keras.io/examples/imdb_bidirectional_lstm/)\n",
        "bi_lstm_no_attention = Sequential()\n",
        "bi_lstm_no_attention.add(Embedding(NUM_CHARS, EMBEDDING_SIZE, input_length=MAX_LENGTH))\n",
        "bi_lstm_no_attention.add(Bidirectional(LSTM(HIDDEN_SIZE)))\n",
        "# bi_lstm_no_attention.add(Dropout(0.5))\n",
        "bi_lstm_no_attention.add(Dense(1, activation='sigmoid'))\n",
        "bi_lstm_no_attention.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(bi_lstm_no_attention.summary())\n",
        "\n",
        "bi_lstm_no_attention.fit(X_train_main, y_train_main,\n",
        "                         validation_data=(X_train_val, y_train_val),\n",
        "                         epochs=3, batch_size=64, class_weight=class_weights_dict)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_12 (Embedding)     (None, 1695, 128)         16384     \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 200)               183200    \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 1)                 201       \n",
            "=================================================================\n",
            "Total params: 199,785\n",
            "Trainable params: 199,785\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 1683 samples, validate on 421 samples\n",
            "Epoch 1/3\n",
            "1683/1683 [==============================] - 151s 90ms/sample - loss: 0.6905 - acc: 0.2561 - val_loss: 0.5329 - val_acc: 0.8124\n",
            "Epoch 2/3\n",
            "1683/1683 [==============================] - 150s 89ms/sample - loss: 0.6561 - acc: 0.5758 - val_loss: 0.6608 - val_acc: 0.5796\n",
            "Epoch 3/3\n",
            "1683/1683 [==============================] - 149s 89ms/sample - loss: 0.6433 - acc: 0.5502 - val_loss: 0.7487 - val_acc: 0.4442\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fd7ef803ef0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uw5c9qR7fLxy",
        "colab_type": "code",
        "outputId": "0aaae820-1bd7-4329-ba2e-ca14249ee93d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "predictions_bi_lstm_no_attention = evaluate_metrics(bi_lstm_no_attention, X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision: 0.9688888888888889\n",
            "Recall: 0.44948453608247424\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_1zqjKTDxgh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create LSTM with Attention using Keras Functional API\n",
        "# (https://stackoverflow.com/questions/42918446/how-to-add-an-attention-mechanism-in-keras)\n",
        "def create_lstm_with_attention():\n",
        "    _input = Input(shape=[MAX_LENGTH])\n",
        "    embedded = Embedding(\n",
        "            input_dim=NUM_CHARS,\n",
        "            output_dim=EMBEDDING_SIZE,\n",
        "            input_length=MAX_LENGTH,\n",
        "            trainable=False,\n",
        "            mask_zero=False\n",
        "        )(_input)\n",
        "    activations = LSTM(HIDDEN_SIZE, return_sequences=True)(embedded)\n",
        "    # activations = Dropout(0.5)(activations)\n",
        "    \n",
        "    # Attention\n",
        "    attention = Dense(1, activation='tanh')(activations) # 'softmax'\n",
        "    # attention = Dense(1)(attention)\n",
        "    attention = Flatten()(attention)\n",
        "    attention = Activation('softmax')(attention)\n",
        "    attention = RepeatVector(HIDDEN_SIZE)(attention)\n",
        "    attention = Permute([2, 1])(attention)\n",
        "    output_attention = multiply([activations, attention])\n",
        "\n",
        "    # output_attention = LSTM(HIDDEN_SIZE)(output_attention)\n",
        "\n",
        "    output_attention = Lambda(lambda xin: K.sum(xin, axis=-2), output_shape=(HIDDEN_SIZE,))(output_attention)\n",
        "    output = Dense(1, activation='sigmoid')(output_attention)\n",
        "    lstm_attention = Model(inputs=[_input], outputs=output, name=\"lstm_attention\")\n",
        "    return lstm_attention"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPwsefSb0atf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print_weights = LambdaCallback(on_epoch_end=lambda epoch, logs: print(lstm_attention.layers[4].get_weights()))\n",
        "print_weights_2 = LambdaCallback(on_epoch_end=lambda epoch, logs: print(lstm_attention.layers[10].get_weights()))\n",
        "\n",
        "print_output = LambdaCallback(on_epoch_end=lambda epoch, logs: print(lstm_attention.layers[6].output))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2V3PDpKa-Qrz",
        "colab_type": "code",
        "outputId": "8e7d3f68-8496-40a9-dd31-424296e3035a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "lstm_attention = create_lstm_with_attention()\n",
        "# lstm_attention.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "sgd = optimizers.SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "lstm_attention.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "\n",
        "print(lstm_attention.summary())\n",
        "\n",
        "lstm_attention.fit(X_train_main, y_train_main,\n",
        "                         validation_data=(X_train_val, y_train_val),\n",
        "                         epochs=5, batch_size=1024, class_weight=class_weights_dict,\n",
        "                   callbacks = [print_weights, print_weights_2])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"lstm_attention\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 1695)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 1695, 64)     8192        input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 1695, 150)    129000      embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 1695, 1)      151         lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 1695)         0           dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 1695)         0           flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "repeat_vector_1 (RepeatVector)  (None, 150, 1695)    0           activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 1695, 150)    0           repeat_vector_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "multiply_1 (Multiply)           (None, 1695, 150)    0           lstm_1[0][0]                     \n",
            "                                                                 permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 150)          0           multiply_1[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 1)            151         lambda_1[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 137,494\n",
            "Trainable params: 129,302\n",
            "Non-trainable params: 8,192\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Train on 1683 samples, validate on 421 samples\n",
            "Epoch 1/5\n",
            "1024/1683 [=================>............] - ETA: 2s - loss: 0.7092 - acc: 0.9121[]\n",
            "[array([[-8.10631290e-02],\n",
            "       [ 1.53723896e-01],\n",
            "       [ 7.42908642e-02],\n",
            "       [-3.05637205e-03],\n",
            "       [-1.13742165e-01],\n",
            "       [ 1.63526595e-01],\n",
            "       [ 1.07596941e-01],\n",
            "       [ 1.16635337e-01],\n",
            "       [ 3.94739360e-02],\n",
            "       [ 2.17249361e-03],\n",
            "       [ 1.06462657e-01],\n",
            "       [ 7.69888237e-02],\n",
            "       [ 1.20317578e-01],\n",
            "       [-1.80867001e-01],\n",
            "       [-1.35893375e-01],\n",
            "       [ 1.31962106e-01],\n",
            "       [ 1.69266418e-01],\n",
            "       [-1.40757725e-01],\n",
            "       [ 5.39501151e-03],\n",
            "       [-1.60007030e-01],\n",
            "       [ 1.59273148e-01],\n",
            "       [ 9.95243639e-02],\n",
            "       [-9.72088128e-02],\n",
            "       [ 1.53843433e-01],\n",
            "       [-1.18658692e-01],\n",
            "       [ 3.35468948e-02],\n",
            "       [-1.92939296e-01],\n",
            "       [-9.68565270e-02],\n",
            "       [ 4.27026898e-02],\n",
            "       [-8.17665458e-02],\n",
            "       [-1.09799698e-01],\n",
            "       [ 1.73908204e-01],\n",
            "       [-3.09238303e-02],\n",
            "       [-1.62640959e-01],\n",
            "       [ 1.26368135e-01],\n",
            "       [ 1.87144607e-01],\n",
            "       [-6.86960761e-03],\n",
            "       [ 1.90793365e-01],\n",
            "       [ 7.98377581e-03],\n",
            "       [-1.79491844e-02],\n",
            "       [-1.08119316e-01],\n",
            "       [ 1.76848322e-01],\n",
            "       [-5.04486375e-02],\n",
            "       [ 1.89930514e-01],\n",
            "       [-5.08858543e-03],\n",
            "       [ 2.16644555e-02],\n",
            "       [ 1.95707768e-01],\n",
            "       [-7.18600303e-02],\n",
            "       [-1.18376963e-01],\n",
            "       [-1.69144526e-01],\n",
            "       [-4.14518733e-03],\n",
            "       [ 1.26203775e-01],\n",
            "       [-4.72167954e-02],\n",
            "       [ 1.96627110e-01],\n",
            "       [ 6.75065815e-02],\n",
            "       [-2.48309728e-02],\n",
            "       [ 1.77260816e-01],\n",
            "       [-1.40634447e-01],\n",
            "       [-1.43478513e-01],\n",
            "       [ 4.17630263e-02],\n",
            "       [-1.51521236e-01],\n",
            "       [ 9.89668816e-02],\n",
            "       [ 1.03402257e-01],\n",
            "       [-1.82678774e-01],\n",
            "       [-1.84882998e-01],\n",
            "       [-3.34411487e-02],\n",
            "       [-1.49805769e-01],\n",
            "       [ 6.84812069e-02],\n",
            "       [ 7.07288235e-02],\n",
            "       [-4.08495851e-02],\n",
            "       [ 1.23973534e-01],\n",
            "       [ 6.83397204e-02],\n",
            "       [ 1.71291471e-01],\n",
            "       [-1.81633785e-01],\n",
            "       [ 1.27501562e-01],\n",
            "       [ 1.59851864e-01],\n",
            "       [ 8.12686905e-02],\n",
            "       [ 1.98926970e-01],\n",
            "       [-1.57811314e-01],\n",
            "       [-1.66124851e-01],\n",
            "       [-6.07285053e-02],\n",
            "       [-1.48742735e-01],\n",
            "       [-1.47507638e-01],\n",
            "       [-1.80851266e-01],\n",
            "       [-1.38055300e-02],\n",
            "       [ 1.10771842e-01],\n",
            "       [ 1.36067703e-01],\n",
            "       [-1.37503296e-01],\n",
            "       [-1.06345318e-01],\n",
            "       [ 1.19362615e-01],\n",
            "       [ 1.92883000e-01],\n",
            "       [ 9.73988846e-02],\n",
            "       [-8.13797563e-02],\n",
            "       [ 1.93101749e-01],\n",
            "       [-1.81767702e-01],\n",
            "       [-1.50892168e-01],\n",
            "       [ 8.79152715e-02],\n",
            "       [-7.26756155e-02],\n",
            "       [-7.41257221e-02],\n",
            "       [-8.43154863e-02],\n",
            "       [-1.80911660e-01],\n",
            "       [ 9.16913524e-02],\n",
            "       [-1.53283745e-01],\n",
            "       [ 5.32241203e-02],\n",
            "       [-1.54168308e-01],\n",
            "       [-7.32618570e-02],\n",
            "       [ 9.10752416e-02],\n",
            "       [-1.84020072e-01],\n",
            "       [-9.56047177e-02],\n",
            "       [ 1.38692722e-01],\n",
            "       [ 4.50522937e-02],\n",
            "       [-1.11187801e-01],\n",
            "       [-1.65960446e-01],\n",
            "       [-4.52674739e-02],\n",
            "       [ 4.71979007e-02],\n",
            "       [-1.87032819e-01],\n",
            "       [ 1.01699889e-01],\n",
            "       [-1.71909872e-02],\n",
            "       [ 1.88656747e-01],\n",
            "       [-1.02246299e-01],\n",
            "       [-1.83751553e-01],\n",
            "       [ 4.74466123e-02],\n",
            "       [-4.33621071e-02],\n",
            "       [ 1.12229958e-01],\n",
            "       [-5.91367111e-02],\n",
            "       [-8.99057463e-02],\n",
            "       [ 8.67227316e-02],\n",
            "       [-1.03345215e-01],\n",
            "       [-4.84105535e-02],\n",
            "       [ 1.77589908e-01],\n",
            "       [-2.47592703e-02],\n",
            "       [ 1.26007944e-04],\n",
            "       [ 6.76543638e-02],\n",
            "       [ 1.80522874e-01],\n",
            "       [-6.78888187e-02],\n",
            "       [ 1.60456374e-01],\n",
            "       [ 1.01034977e-01],\n",
            "       [-5.75465113e-02],\n",
            "       [-1.39048651e-01],\n",
            "       [-1.32326949e-02],\n",
            "       [ 9.71825421e-02],\n",
            "       [ 8.03670436e-02],\n",
            "       [ 6.13616034e-02],\n",
            "       [-1.31373629e-01],\n",
            "       [ 1.10190600e-01],\n",
            "       [ 1.89344019e-01],\n",
            "       [ 1.12028994e-01],\n",
            "       [-1.79195568e-01],\n",
            "       [-1.05390940e-02],\n",
            "       [-1.39197066e-01]], dtype=float32), array([0.00760852], dtype=float32)]\n",
            "1683/1683 [==============================] - 7s 4ms/sample - loss: 0.7115 - acc: 0.5906 - val_loss: 0.7011 - val_acc: 0.0641\n",
            "Epoch 2/5\n",
            "1024/1683 [=================>............] - ETA: 1s - loss: 0.7306 - acc: 0.0938[]\n",
            "[array([[-8.09070617e-02],\n",
            "       [ 1.53791741e-01],\n",
            "       [ 7.43540749e-02],\n",
            "       [-3.30059952e-03],\n",
            "       [-1.13674596e-01],\n",
            "       [ 1.63652807e-01],\n",
            "       [ 1.07762337e-01],\n",
            "       [ 1.17002070e-01],\n",
            "       [ 3.94362472e-02],\n",
            "       [ 2.16665003e-03],\n",
            "       [ 1.06346264e-01],\n",
            "       [ 7.71028623e-02],\n",
            "       [ 1.20293178e-01],\n",
            "       [-1.80912808e-01],\n",
            "       [-1.35816470e-01],\n",
            "       [ 1.31806582e-01],\n",
            "       [ 1.69226840e-01],\n",
            "       [-1.40706688e-01],\n",
            "       [ 5.36880456e-03],\n",
            "       [-1.59933940e-01],\n",
            "       [ 1.59365192e-01],\n",
            "       [ 9.95908603e-02],\n",
            "       [-9.72751006e-02],\n",
            "       [ 1.53935179e-01],\n",
            "       [-1.18561149e-01],\n",
            "       [ 3.37350816e-02],\n",
            "       [-1.92813858e-01],\n",
            "       [-9.69500467e-02],\n",
            "       [ 4.26294580e-02],\n",
            "       [-8.18278790e-02],\n",
            "       [-1.09661311e-01],\n",
            "       [ 1.73665464e-01],\n",
            "       [-3.10419779e-02],\n",
            "       [-1.62447214e-01],\n",
            "       [ 1.26304492e-01],\n",
            "       [ 1.87210515e-01],\n",
            "       [-6.99301343e-03],\n",
            "       [ 1.90814570e-01],\n",
            "       [ 7.84740411e-03],\n",
            "       [-1.78397000e-02],\n",
            "       [-1.08356662e-01],\n",
            "       [ 1.76826388e-01],\n",
            "       [-5.04243486e-02],\n",
            "       [ 1.89917088e-01],\n",
            "       [-5.08470368e-03],\n",
            "       [ 2.16227435e-02],\n",
            "       [ 1.95575297e-01],\n",
            "       [-7.16716573e-02],\n",
            "       [-1.18515886e-01],\n",
            "       [-1.69245899e-01],\n",
            "       [-4.07848554e-03],\n",
            "       [ 1.26179144e-01],\n",
            "       [-4.70424704e-02],\n",
            "       [ 1.96726754e-01],\n",
            "       [ 6.77077696e-02],\n",
            "       [-2.49192417e-02],\n",
            "       [ 1.77400529e-01],\n",
            "       [-1.40510112e-01],\n",
            "       [-1.43411577e-01],\n",
            "       [ 4.18944918e-02],\n",
            "       [-1.51480049e-01],\n",
            "       [ 9.88621712e-02],\n",
            "       [ 1.03423581e-01],\n",
            "       [-1.82693064e-01],\n",
            "       [-1.84732422e-01],\n",
            "       [-3.36039662e-02],\n",
            "       [-1.49927735e-01],\n",
            "       [ 6.85749054e-02],\n",
            "       [ 7.05307350e-02],\n",
            "       [-4.09160592e-02],\n",
            "       [ 1.23873934e-01],\n",
            "       [ 6.85850829e-02],\n",
            "       [ 1.71337083e-01],\n",
            "       [-1.81548372e-01],\n",
            "       [ 1.27784699e-01],\n",
            "       [ 1.59770712e-01],\n",
            "       [ 8.13078880e-02],\n",
            "       [ 1.98961839e-01],\n",
            "       [-1.57766804e-01],\n",
            "       [-1.66201517e-01],\n",
            "       [-6.06996641e-02],\n",
            "       [-1.48876026e-01],\n",
            "       [-1.47315517e-01],\n",
            "       [-1.80730283e-01],\n",
            "       [-1.37607316e-02],\n",
            "       [ 1.10637315e-01],\n",
            "       [ 1.36014193e-01],\n",
            "       [-1.37637392e-01],\n",
            "       [-1.06255800e-01],\n",
            "       [ 1.19424857e-01],\n",
            "       [ 1.92603707e-01],\n",
            "       [ 9.74436402e-02],\n",
            "       [-8.16403553e-02],\n",
            "       [ 1.93298936e-01],\n",
            "       [-1.81861520e-01],\n",
            "       [-1.50770694e-01],\n",
            "       [ 8.79068375e-02],\n",
            "       [-7.26797953e-02],\n",
            "       [-7.40817413e-02],\n",
            "       [-8.42799395e-02],\n",
            "       [-1.81045115e-01],\n",
            "       [ 9.15556103e-02],\n",
            "       [-1.53423935e-01],\n",
            "       [ 5.29980697e-02],\n",
            "       [-1.54212505e-01],\n",
            "       [-7.30374530e-02],\n",
            "       [ 9.12280977e-02],\n",
            "       [-1.83968037e-01],\n",
            "       [-9.57133621e-02],\n",
            "       [ 1.38814196e-01],\n",
            "       [ 4.52904478e-02],\n",
            "       [-1.11143343e-01],\n",
            "       [-1.65891051e-01],\n",
            "       [-4.51801270e-02],\n",
            "       [ 4.72328626e-02],\n",
            "       [-1.87042698e-01],\n",
            "       [ 1.01737380e-01],\n",
            "       [-1.71206724e-02],\n",
            "       [ 1.88746095e-01],\n",
            "       [-1.02202900e-01],\n",
            "       [-1.83684766e-01],\n",
            "       [ 4.73672710e-02],\n",
            "       [-4.34363820e-02],\n",
            "       [ 1.12307549e-01],\n",
            "       [-5.90487234e-02],\n",
            "       [-8.99283215e-02],\n",
            "       [ 8.67681056e-02],\n",
            "       [-1.03319898e-01],\n",
            "       [-4.83975857e-02],\n",
            "       [ 1.77508950e-01],\n",
            "       [-2.48349179e-02],\n",
            "       [ 1.88971899e-04],\n",
            "       [ 6.78424239e-02],\n",
            "       [ 1.80484280e-01],\n",
            "       [-6.80149421e-02],\n",
            "       [ 1.60326838e-01],\n",
            "       [ 1.01145975e-01],\n",
            "       [-5.76050505e-02],\n",
            "       [-1.38942599e-01],\n",
            "       [-1.30359558e-02],\n",
            "       [ 9.71905962e-02],\n",
            "       [ 8.04042816e-02],\n",
            "       [ 6.12245724e-02],\n",
            "       [-1.31311998e-01],\n",
            "       [ 1.10108465e-01],\n",
            "       [ 1.89359501e-01],\n",
            "       [ 1.11914866e-01],\n",
            "       [-1.79298058e-01],\n",
            "       [-1.02671171e-02],\n",
            "       [-1.39056206e-01]], dtype=float32), array([0.01617268], dtype=float32)]\n",
            "1683/1683 [==============================] - 6s 4ms/sample - loss: 0.7114 - acc: 0.0885 - val_loss: 0.7129 - val_acc: 0.0641\n",
            "Epoch 3/5\n",
            "1024/1683 [=================>............] - ETA: 1s - loss: 0.6805 - acc: 0.0801[]\n",
            "[array([[-0.08075798],\n",
            "       [ 0.1538632 ],\n",
            "       [ 0.07441403],\n",
            "       [-0.0035332 ],\n",
            "       [-0.11360917],\n",
            "       [ 0.16378014],\n",
            "       [ 0.10792306],\n",
            "       [ 0.11735538],\n",
            "       [ 0.03939928],\n",
            "       [ 0.00216564],\n",
            "       [ 0.10623923],\n",
            "       [ 0.07721195],\n",
            "       [ 0.12027192],\n",
            "       [-0.18095559],\n",
            "       [-0.13574657],\n",
            "       [ 0.13166255],\n",
            "       [ 0.16919146],\n",
            "       [-0.14065923],\n",
            "       [ 0.00534217],\n",
            "       [-0.15986712],\n",
            "       [ 0.15945466],\n",
            "       [ 0.09965854],\n",
            "       [-0.09733916],\n",
            "       [ 0.15402447],\n",
            "       [-0.11847056],\n",
            "       [ 0.03391625],\n",
            "       [-0.192699  ],\n",
            "       [-0.09703737],\n",
            "       [ 0.04256213],\n",
            "       [-0.08188483],\n",
            "       [-0.10953441],\n",
            "       [ 0.17343646],\n",
            "       [-0.03115317],\n",
            "       [-0.162265  ],\n",
            "       [ 0.12624332],\n",
            "       [ 0.1872763 ],\n",
            "       [-0.00711252],\n",
            "       [ 0.19083624],\n",
            "       [ 0.00771856],\n",
            "       [-0.01773792],\n",
            "       [-0.10858396],\n",
            "       [ 0.17680895],\n",
            "       [-0.05040422],\n",
            "       [ 0.18990947],\n",
            "       [-0.00508023],\n",
            "       [ 0.02158184],\n",
            "       [ 0.19545051],\n",
            "       [-0.07149623],\n",
            "       [-0.11864834],\n",
            "       [-0.16934483],\n",
            "       [-0.0040145 ],\n",
            "       [ 0.12615675],\n",
            "       [-0.04687627],\n",
            "       [ 0.19682367],\n",
            "       [ 0.06790337],\n",
            "       [-0.02500553],\n",
            "       [ 0.17753707],\n",
            "       [-0.14039467],\n",
            "       [-0.14335218],\n",
            "       [ 0.0420237 ],\n",
            "       [-0.15144555],\n",
            "       [ 0.09876309],\n",
            "       [ 0.1034456 ],\n",
            "       [-0.1827096 ],\n",
            "       [-0.18459027],\n",
            "       [-0.03375774],\n",
            "       [-0.1500485 ],\n",
            "       [ 0.06866715],\n",
            "       [ 0.07034226],\n",
            "       [-0.04097874],\n",
            "       [ 0.12378579],\n",
            "       [ 0.06882147],\n",
            "       [ 0.17137815],\n",
            "       [-0.18147059],\n",
            "       [ 0.12805536],\n",
            "       [ 0.15969482],\n",
            "       [ 0.08135092],\n",
            "       [ 0.19899833],\n",
            "       [-0.15772901],\n",
            "       [-0.16627827],\n",
            "       [-0.060676  ],\n",
            "       [-0.14900427],\n",
            "       [-0.14713483],\n",
            "       [-0.18062188],\n",
            "       [-0.01371903],\n",
            "       [ 0.11051285],\n",
            "       [ 0.13596418],\n",
            "       [-0.13776782],\n",
            "       [-0.10616921],\n",
            "       [ 0.11948264],\n",
            "       [ 0.19233762],\n",
            "       [ 0.09748525],\n",
            "       [-0.08188789],\n",
            "       [ 0.19349223],\n",
            "       [-0.18195294],\n",
            "       [-0.1506549 ],\n",
            "       [ 0.08789791],\n",
            "       [-0.07268217],\n",
            "       [-0.07403967],\n",
            "       [-0.08424243],\n",
            "       [-0.18117271],\n",
            "       [ 0.09142901],\n",
            "       [-0.15356077],\n",
            "       [ 0.05278285],\n",
            "       [-0.154262  ],\n",
            "       [-0.07282744],\n",
            "       [ 0.09137559],\n",
            "       [-0.1839185 ],\n",
            "       [-0.095819  ],\n",
            "       [ 0.13893217],\n",
            "       [ 0.0455173 ],\n",
            "       [-0.11110558],\n",
            "       [-0.16582872],\n",
            "       [-0.04509194],\n",
            "       [ 0.047264  ],\n",
            "       [-0.18705167],\n",
            "       [ 0.10177267],\n",
            "       [-0.01705508],\n",
            "       [ 0.18883485],\n",
            "       [-0.10216487],\n",
            "       [-0.18362823],\n",
            "       [ 0.04729074],\n",
            "       [-0.04350788],\n",
            "       [ 0.11238562],\n",
            "       [-0.05896382],\n",
            "       [-0.08994965],\n",
            "       [ 0.08680751],\n",
            "       [-0.10329785],\n",
            "       [-0.04838575],\n",
            "       [ 0.17743681],\n",
            "       [-0.0249052 ],\n",
            "       [ 0.00024658],\n",
            "       [ 0.06802313],\n",
            "       [ 0.18044774],\n",
            "       [-0.06813478],\n",
            "       [ 0.16020432],\n",
            "       [ 0.1012509 ],\n",
            "       [-0.05766495],\n",
            "       [-0.13884528],\n",
            "       [-0.01284905],\n",
            "       [ 0.09719832],\n",
            "       [ 0.08044017],\n",
            "       [ 0.06109853],\n",
            "       [-0.13125636],\n",
            "       [ 0.11003282],\n",
            "       [ 0.18937607],\n",
            "       [ 0.11180627],\n",
            "       [-0.17940196],\n",
            "       [-0.01001036],\n",
            "       [-0.13892414]], dtype=float32), array([0.02380991], dtype=float32)]\n",
            "1683/1683 [==============================] - 6s 4ms/sample - loss: 0.7112 - acc: 0.0885 - val_loss: 0.7237 - val_acc: 0.0641\n",
            "Epoch 4/5\n",
            "1024/1683 [=================>............] - ETA: 2s - loss: 0.7053 - acc: 0.0869[]\n",
            "[array([[-0.08061413],\n",
            "       [ 0.15393549],\n",
            "       [ 0.07447178],\n",
            "       [-0.00375603],\n",
            "       [-0.1135443 ],\n",
            "       [ 0.16390526],\n",
            "       [ 0.10807898],\n",
            "       [ 0.11769579],\n",
            "       [ 0.03936313],\n",
            "       [ 0.00216821],\n",
            "       [ 0.10613845],\n",
            "       [ 0.07731687],\n",
            "       [ 0.12025237],\n",
            "       [-0.18099348],\n",
            "       [-0.13568078],\n",
            "       [ 0.13152604],\n",
            "       [ 0.16915822],\n",
            "       [-0.14061366],\n",
            "       [ 0.00531556],\n",
            "       [-0.1598046 ],\n",
            "       [ 0.15954027],\n",
            "       [ 0.09972492],\n",
            "       [-0.09739985],\n",
            "       [ 0.15411034],\n",
            "       [-0.11838497],\n",
            "       [ 0.03409017],\n",
            "       [-0.19259037],\n",
            "       [-0.09711972],\n",
            "       [ 0.04249882],\n",
            "       [-0.08193817],\n",
            "       [-0.10941584],\n",
            "       [ 0.1732176 ],\n",
            "       [-0.03125903],\n",
            "       [-0.16209097],\n",
            "       [ 0.12618338],\n",
            "       [ 0.18733981],\n",
            "       [-0.00722801],\n",
            "       [ 0.19085716],\n",
            "       [ 0.00759546],\n",
            "       [-0.01764215],\n",
            "       [-0.10880176],\n",
            "       [ 0.17679337],\n",
            "       [-0.05038633],\n",
            "       [ 0.1899043 ],\n",
            "       [-0.00507422],\n",
            "       [ 0.02154107],\n",
            "       [ 0.1953304 ],\n",
            "       [-0.07133072],\n",
            "       [-0.11877497],\n",
            "       [-0.16944005],\n",
            "       [-0.0039534 ],\n",
            "       [ 0.1261354 ],\n",
            "       [-0.04671637],\n",
            "       [ 0.1969166 ],\n",
            "       [ 0.06809378],\n",
            "       [-0.02508944],\n",
            "       [ 0.17766936],\n",
            "       [-0.14028542],\n",
            "       [-0.14329784],\n",
            "       [ 0.04214972],\n",
            "       [-0.15141502],\n",
            "       [ 0.09866773],\n",
            "       [ 0.10346673],\n",
            "       [-0.18272619],\n",
            "       [-0.18445405],\n",
            "       [-0.03390408],\n",
            "       [-0.15016748],\n",
            "       [ 0.06875782],\n",
            "       [ 0.07016079],\n",
            "       [-0.04103854],\n",
            "       [ 0.12370542],\n",
            "       [ 0.06905028],\n",
            "       [ 0.17141587],\n",
            "       [-0.18139705],\n",
            "       [ 0.12831551],\n",
            "       [ 0.1596226 ],\n",
            "       [ 0.08139502],\n",
            "       [ 0.19903453],\n",
            "       [-0.15769479],\n",
            "       [-0.1663529 ],\n",
            "       [-0.06065563],\n",
            "       [-0.14912906],\n",
            "       [-0.1469621 ],\n",
            "       [-0.18052097],\n",
            "       [-0.01367973],\n",
            "       [ 0.11039501],\n",
            "       [ 0.13591757],\n",
            "       [-0.13789514],\n",
            "       [-0.10608542],\n",
            "       [ 0.11953687],\n",
            "       [ 0.19208159],\n",
            "       [ 0.09752322],\n",
            "       [-0.08212522],\n",
            "       [ 0.1936799 ],\n",
            "       [-0.18204062],\n",
            "       [-0.15054274],\n",
            "       [ 0.08788755],\n",
            "       [-0.07268322],\n",
            "       [-0.07399926],\n",
            "       [-0.08420403],\n",
            "       [-0.18129475],\n",
            "       [ 0.09130845],\n",
            "       [-0.15369248],\n",
            "       [ 0.05257645],\n",
            "       [-0.15431331],\n",
            "       [-0.07262696],\n",
            "       [ 0.09151818],\n",
            "       [-0.18387032],\n",
            "       [-0.09592006],\n",
            "       [ 0.1390453 ],\n",
            "       [ 0.04573464],\n",
            "       [-0.11107251],\n",
            "       [-0.16577032],\n",
            "       [-0.04500402],\n",
            "       [ 0.04729231],\n",
            "       [-0.18705866],\n",
            "       [ 0.10180624],\n",
            "       [-0.01699383],\n",
            "       [ 0.18892069],\n",
            "       [-0.10213004],\n",
            "       [-0.18357812],\n",
            "       [ 0.04721633],\n",
            "       [-0.04357722],\n",
            "       [ 0.11246245],\n",
            "       [-0.05888152],\n",
            "       [-0.08996949],\n",
            "       [ 0.08684195],\n",
            "       [-0.10327737],\n",
            "       [-0.04837445],\n",
            "       [ 0.17736857],\n",
            "       [-0.02497142],\n",
            "       [ 0.00030149],\n",
            "       [ 0.0681968 ],\n",
            "       [ 0.18041244],\n",
            "       [-0.0682487 ],\n",
            "       [ 0.16008745],\n",
            "       [ 0.10134958],\n",
            "       [-0.05772454],\n",
            "       [-0.13875353],\n",
            "       [-0.01267012],\n",
            "       [ 0.09720532],\n",
            "       [ 0.08047464],\n",
            "       [ 0.06098003],\n",
            "       [-0.13120434],\n",
            "       [ 0.10996217],\n",
            "       [ 0.1893918 ],\n",
            "       [ 0.11170117],\n",
            "       [-0.17950484],\n",
            "       [-0.00976634],\n",
            "       [-0.13879822]], dtype=float32), array([0.03070807], dtype=float32)]\n",
            "1683/1683 [==============================] - 6s 4ms/sample - loss: 0.7110 - acc: 0.0885 - val_loss: 0.7336 - val_acc: 0.0641\n",
            "Epoch 5/5\n",
            "1024/1683 [=================>............] - ETA: 2s - loss: 0.6953 - acc: 0.0840[]\n",
            "[array([[-0.08051408],\n",
            "       [ 0.15397224],\n",
            "       [ 0.07451326],\n",
            "       [-0.00389653],\n",
            "       [-0.11348662],\n",
            "       [ 0.16396396],\n",
            "       [ 0.1081741 ],\n",
            "       [ 0.11790273],\n",
            "       [ 0.03934195],\n",
            "       [ 0.00217535],\n",
            "       [ 0.10606425],\n",
            "       [ 0.07738595],\n",
            "       [ 0.12023406],\n",
            "       [-0.18098909],\n",
            "       [-0.13562584],\n",
            "       [ 0.13142735],\n",
            "       [ 0.16912827],\n",
            "       [-0.14057475],\n",
            "       [ 0.00529881],\n",
            "       [-0.15975854],\n",
            "       [ 0.15958075],\n",
            "       [ 0.09975268],\n",
            "       [-0.09742618],\n",
            "       [ 0.15415269],\n",
            "       [-0.11833023],\n",
            "       [ 0.03419005],\n",
            "       [-0.1925034 ],\n",
            "       [-0.097171  ],\n",
            "       [ 0.042454  ],\n",
            "       [-0.08196895],\n",
            "       [-0.10933656],\n",
            "       [ 0.17306879],\n",
            "       [-0.03132933],\n",
            "       [-0.16196927],\n",
            "       [ 0.12613544],\n",
            "       [ 0.18736325],\n",
            "       [-0.00730103],\n",
            "       [ 0.19086285],\n",
            "       [ 0.00751295],\n",
            "       [-0.01758162],\n",
            "       [-0.10893166],\n",
            "       [ 0.17677349],\n",
            "       [-0.05036638],\n",
            "       [ 0.18989024],\n",
            "       [-0.00505855],\n",
            "       [ 0.02150534],\n",
            "       [ 0.1952401 ],\n",
            "       [-0.07122404],\n",
            "       [-0.11884952],\n",
            "       [-0.16948956],\n",
            "       [-0.00391556],\n",
            "       [ 0.12611423],\n",
            "       [-0.04660723],\n",
            "       [ 0.19695921],\n",
            "       [ 0.06822042],\n",
            "       [-0.02514224],\n",
            "       [ 0.1777444 ],\n",
            "       [-0.1402098 ],\n",
            "       [-0.14325821],\n",
            "       [ 0.04222459],\n",
            "       [-0.15139   ],\n",
            "       [ 0.09859853],\n",
            "       [ 0.10346838],\n",
            "       [-0.1827244 ],\n",
            "       [-0.1843596 ],\n",
            "       [-0.0339932 ],\n",
            "       [-0.15024902],\n",
            "       [ 0.06882011],\n",
            "       [ 0.07003565],\n",
            "       [-0.04107478],\n",
            "       [ 0.12365001],\n",
            "       [ 0.0692022 ],\n",
            "       [ 0.17144015],\n",
            "       [-0.18133527],\n",
            "       [ 0.12848462],\n",
            "       [ 0.15957265],\n",
            "       [ 0.08141372],\n",
            "       [ 0.19904858],\n",
            "       [-0.15766303],\n",
            "       [-0.16638409],\n",
            "       [-0.06063848],\n",
            "       [-0.14921813],\n",
            "       [-0.14683965],\n",
            "       [-0.18043949],\n",
            "       [-0.01365295],\n",
            "       [ 0.11031024],\n",
            "       [ 0.13589528],\n",
            "       [-0.13798143],\n",
            "       [-0.10603321],\n",
            "       [ 0.11957096],\n",
            "       [ 0.19190605],\n",
            "       [ 0.09753202],\n",
            "       [-0.08227992],\n",
            "       [ 0.19378999],\n",
            "       [-0.1820793 ],\n",
            "       [-0.15045746],\n",
            "       [ 0.08786543],\n",
            "       [-0.07268131],\n",
            "       [-0.07396884],\n",
            "       [-0.08417739],\n",
            "       [-0.18136637],\n",
            "       [ 0.09121974],\n",
            "       [-0.1537622 ],\n",
            "       [ 0.05244189],\n",
            "       [-0.15433699],\n",
            "       [-0.07247669],\n",
            "       [ 0.09161009],\n",
            "       [-0.18382834],\n",
            "       [-0.09596889],\n",
            "       [ 0.13910352],\n",
            "       [ 0.04587347],\n",
            "       [-0.11105165],\n",
            "       [-0.16572046],\n",
            "       [-0.0449425 ],\n",
            "       [ 0.04730908],\n",
            "       [-0.18704534],\n",
            "       [ 0.10182817],\n",
            "       [-0.01696245],\n",
            "       [ 0.18895654],\n",
            "       [-0.10209895],\n",
            "       [-0.1835361 ],\n",
            "       [ 0.04716229],\n",
            "       [-0.0436215 ],\n",
            "       [ 0.1125042 ],\n",
            "       [-0.05882836],\n",
            "       [-0.08997642],\n",
            "       [ 0.08685695],\n",
            "       [-0.10325485],\n",
            "       [-0.04836184],\n",
            "       [ 0.1772998 ],\n",
            "       [-0.0250139 ],\n",
            "       [ 0.00034896],\n",
            "       [ 0.06830252],\n",
            "       [ 0.18038563],\n",
            "       [-0.06831288],\n",
            "       [ 0.16001016],\n",
            "       [ 0.10140066],\n",
            "       [-0.0577563 ],\n",
            "       [-0.13868241],\n",
            "       [-0.01255374],\n",
            "       [ 0.09720202],\n",
            "       [ 0.08049335],\n",
            "       [ 0.0608971 ],\n",
            "       [-0.1311585 ],\n",
            "       [ 0.10991808],\n",
            "       [ 0.18938611],\n",
            "       [ 0.11162318],\n",
            "       [-0.17956278],\n",
            "       [-0.00961715],\n",
            "       [-0.1387105 ]], dtype=float32), array([0.03396791], dtype=float32)]\n",
            "1683/1683 [==============================] - 6s 4ms/sample - loss: 0.7111 - acc: 0.0885 - val_loss: 0.7384 - val_acc: 0.0641\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f43fb0ff470>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RberrL7ox-L",
        "colab_type": "code",
        "outputId": "85cfb0d5-0113-4b11-d726-facfb9ae2451",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        }
      },
      "source": [
        "predictions_lstm_attention, rounded_lstm_attention = evaluate_metrics(lstm_attention, X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ZeroDivisionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-7deaa93a1be1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictions_lstm_attention\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrounded_lstm_attention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_precision_recall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_attention\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-bbf39f5be2db>\u001b[0m in \u001b[0;36mevaluate_precision_recall\u001b[0;34m(model, X_test, y_test)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mrounded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;31m# predictions = model.predict_classes(X_test)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Precision: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrounded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Recall: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrounded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrounded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-bbf39f5be2db>\u001b[0m in \u001b[0;36mprecision\u001b[0;34m(decoded_vals, target_vals)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mFP\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoded_vals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtarget_vals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mTP\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTP\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mFP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# Recall = TP / (TP+FN).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQWMEw2zzo7G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions_lstm_attention"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UJeldgJCn8L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rounded_lstm_attention"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVknY95v_sFy",
        "colab_type": "code",
        "outputId": "a6ed3672-3509-4949-d09d-01f8be172ad9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "plt.hist(predictions_lstm_attention)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD4CAYAAADlwTGnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2FaNuJKf_sG",
        "colab_type": "code",
        "outputId": "aee53504-c172-4f72-dbb2-4eac3f11cece",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "predictions_lstm_attention"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.4961562 ],\n",
              "       [0.49609172],\n",
              "       [0.49615976],\n",
              "       [0.49607486],\n",
              "       [0.4961688 ],\n",
              "       [0.49612573],\n",
              "       [0.49614757],\n",
              "       [0.4961554 ],\n",
              "       [0.4961159 ],\n",
              "       [0.49597448],\n",
              "       [0.49617523],\n",
              "       [0.49607423],\n",
              "       [0.49605054],\n",
              "       [0.4961135 ],\n",
              "       [0.49616486],\n",
              "       [0.49596468],\n",
              "       [0.4961772 ],\n",
              "       [0.49618816],\n",
              "       [0.49549094],\n",
              "       [0.49612513],\n",
              "       [0.4961008 ],\n",
              "       [0.4961989 ],\n",
              "       [0.49613893],\n",
              "       [0.49606025],\n",
              "       [0.49615124],\n",
              "       [0.49605876],\n",
              "       [0.4960235 ],\n",
              "       [0.496061  ],\n",
              "       [0.49608633],\n",
              "       [0.49609944],\n",
              "       [0.49620906],\n",
              "       [0.49543896],\n",
              "       [0.49595445],\n",
              "       [0.4960561 ],\n",
              "       [0.49603152],\n",
              "       [0.49607995],\n",
              "       [0.49614912],\n",
              "       [0.49606827],\n",
              "       [0.49612287],\n",
              "       [0.4961007 ],\n",
              "       [0.49611697],\n",
              "       [0.4960754 ],\n",
              "       [0.49570054],\n",
              "       [0.49610198],\n",
              "       [0.49594525],\n",
              "       [0.49615216],\n",
              "       [0.49611413],\n",
              "       [0.4961698 ],\n",
              "       [0.49586254],\n",
              "       [0.49608734],\n",
              "       [0.49582225],\n",
              "       [0.4961073 ],\n",
              "       [0.49612948],\n",
              "       [0.49616912],\n",
              "       [0.4954837 ],\n",
              "       [0.4962128 ],\n",
              "       [0.49611118],\n",
              "       [0.4961866 ],\n",
              "       [0.49588388],\n",
              "       [0.49619395],\n",
              "       [0.49603042],\n",
              "       [0.49611032],\n",
              "       [0.49615306],\n",
              "       [0.4961295 ],\n",
              "       [0.49617103],\n",
              "       [0.49602142],\n",
              "       [0.4959659 ],\n",
              "       [0.49609193],\n",
              "       [0.49611506],\n",
              "       [0.49610505],\n",
              "       [0.4961717 ],\n",
              "       [0.4961366 ],\n",
              "       [0.49599668],\n",
              "       [0.49602768],\n",
              "       [0.49607497],\n",
              "       [0.49593627],\n",
              "       [0.49611983],\n",
              "       [0.49603808],\n",
              "       [0.49614096],\n",
              "       [0.49603584],\n",
              "       [0.49615586],\n",
              "       [0.49606177],\n",
              "       [0.49612606],\n",
              "       [0.49609655],\n",
              "       [0.49619845],\n",
              "       [0.49617147],\n",
              "       [0.49613217],\n",
              "       [0.49612936],\n",
              "       [0.49613675],\n",
              "       [0.49611032],\n",
              "       [0.49607038],\n",
              "       [0.49617037],\n",
              "       [0.49604976],\n",
              "       [0.4962152 ],\n",
              "       [0.49609232],\n",
              "       [0.495885  ],\n",
              "       [0.49599868],\n",
              "       [0.4962005 ],\n",
              "       [0.4961578 ],\n",
              "       [0.49612105],\n",
              "       [0.49589536],\n",
              "       [0.49605966],\n",
              "       [0.4960917 ],\n",
              "       [0.49621314],\n",
              "       [0.49612924],\n",
              "       [0.49618647],\n",
              "       [0.49607193],\n",
              "       [0.49620205],\n",
              "       [0.49616972],\n",
              "       [0.49603721],\n",
              "       [0.49606204],\n",
              "       [0.49620393],\n",
              "       [0.49614838],\n",
              "       [0.4961968 ],\n",
              "       [0.4961233 ],\n",
              "       [0.49620095],\n",
              "       [0.49607196],\n",
              "       [0.4959411 ],\n",
              "       [0.49617472],\n",
              "       [0.49613327],\n",
              "       [0.4960925 ],\n",
              "       [0.49598068],\n",
              "       [0.4960871 ],\n",
              "       [0.49612185],\n",
              "       [0.49615726],\n",
              "       [0.49611542],\n",
              "       [0.49616662],\n",
              "       [0.49615702],\n",
              "       [0.49607816],\n",
              "       [0.49613374],\n",
              "       [0.4961519 ],\n",
              "       [0.49621356],\n",
              "       [0.49609149],\n",
              "       [0.49605328],\n",
              "       [0.49607325],\n",
              "       [0.49595323],\n",
              "       [0.4961401 ],\n",
              "       [0.4960816 ],\n",
              "       [0.4960007 ],\n",
              "       [0.4959944 ],\n",
              "       [0.49618793],\n",
              "       [0.4960427 ],\n",
              "       [0.49605346],\n",
              "       [0.4961319 ],\n",
              "       [0.4960782 ],\n",
              "       [0.49614167],\n",
              "       [0.49593133],\n",
              "       [0.4961997 ],\n",
              "       [0.4961478 ],\n",
              "       [0.4960679 ],\n",
              "       [0.4961174 ],\n",
              "       [0.49607843],\n",
              "       [0.49610034],\n",
              "       [0.49580792],\n",
              "       [0.49615556],\n",
              "       [0.49612966],\n",
              "       [0.4961579 ],\n",
              "       [0.4960074 ],\n",
              "       [0.49614155],\n",
              "       [0.49617353],\n",
              "       [0.49614298],\n",
              "       [0.4962245 ],\n",
              "       [0.49613512],\n",
              "       [0.4960201 ],\n",
              "       [0.49612173],\n",
              "       [0.49609986],\n",
              "       [0.49605215],\n",
              "       [0.49614227],\n",
              "       [0.4959452 ],\n",
              "       [0.49605787],\n",
              "       [0.4959805 ],\n",
              "       [0.49612987],\n",
              "       [0.49594977],\n",
              "       [0.49609894],\n",
              "       [0.4961373 ],\n",
              "       [0.4960538 ],\n",
              "       [0.49601644],\n",
              "       [0.49605235],\n",
              "       [0.4961638 ],\n",
              "       [0.49610618],\n",
              "       [0.49607745],\n",
              "       [0.49605033],\n",
              "       [0.49614787],\n",
              "       [0.49618253],\n",
              "       [0.4961047 ],\n",
              "       [0.49608186],\n",
              "       [0.4959678 ],\n",
              "       [0.49592814],\n",
              "       [0.49600598],\n",
              "       [0.49575475],\n",
              "       [0.4960992 ],\n",
              "       [0.49611315],\n",
              "       [0.49605048],\n",
              "       [0.49611023],\n",
              "       [0.49613965],\n",
              "       [0.49603468],\n",
              "       [0.49595496],\n",
              "       [0.49620536],\n",
              "       [0.49611908],\n",
              "       [0.4961523 ],\n",
              "       [0.49616045],\n",
              "       [0.4953735 ],\n",
              "       [0.49604785],\n",
              "       [0.49593475],\n",
              "       [0.49561876],\n",
              "       [0.49600056],\n",
              "       [0.49611136],\n",
              "       [0.49613348],\n",
              "       [0.49603269],\n",
              "       [0.4960179 ],\n",
              "       [0.49617776],\n",
              "       [0.49601564],\n",
              "       [0.49611202],\n",
              "       [0.49613214],\n",
              "       [0.49591294],\n",
              "       [0.4961391 ],\n",
              "       [0.49601814],\n",
              "       [0.49610838],\n",
              "       [0.49604326],\n",
              "       [0.49586132],\n",
              "       [0.49600542],\n",
              "       [0.4960607 ],\n",
              "       [0.49619702],\n",
              "       [0.49612328],\n",
              "       [0.49603695],\n",
              "       [0.49611846],\n",
              "       [0.49609175],\n",
              "       [0.4961244 ],\n",
              "       [0.49618596],\n",
              "       [0.49601224],\n",
              "       [0.49606118],\n",
              "       [0.49610612],\n",
              "       [0.49617326],\n",
              "       [0.49610218],\n",
              "       [0.49611768],\n",
              "       [0.496123  ],\n",
              "       [0.49568012],\n",
              "       [0.49597588],\n",
              "       [0.49606898],\n",
              "       [0.49555525],\n",
              "       [0.49608776],\n",
              "       [0.49554807],\n",
              "       [0.49616155],\n",
              "       [0.49607715],\n",
              "       [0.4959071 ],\n",
              "       [0.4960875 ],\n",
              "       [0.49605376],\n",
              "       [0.49586394],\n",
              "       [0.49610746],\n",
              "       [0.49610278],\n",
              "       [0.4960941 ],\n",
              "       [0.49605647],\n",
              "       [0.49604037],\n",
              "       [0.49582613],\n",
              "       [0.49591625],\n",
              "       [0.49607491],\n",
              "       [0.49610758],\n",
              "       [0.49614513],\n",
              "       [0.49600023],\n",
              "       [0.4961578 ],\n",
              "       [0.49609238],\n",
              "       [0.49620032],\n",
              "       [0.4962181 ],\n",
              "       [0.49608043],\n",
              "       [0.49607235],\n",
              "       [0.49611893],\n",
              "       [0.49601993],\n",
              "       [0.4960699 ],\n",
              "       [0.49608776],\n",
              "       [0.49607775],\n",
              "       [0.49610606],\n",
              "       [0.49621603],\n",
              "       [0.4958363 ],\n",
              "       [0.49610925],\n",
              "       [0.4961497 ],\n",
              "       [0.49615815],\n",
              "       [0.4959606 ],\n",
              "       [0.49603176],\n",
              "       [0.49615124],\n",
              "       [0.49608725],\n",
              "       [0.49617594],\n",
              "       [0.4960998 ],\n",
              "       [0.4960858 ],\n",
              "       [0.4960732 ],\n",
              "       [0.49602526],\n",
              "       [0.496165  ],\n",
              "       [0.49612507],\n",
              "       [0.4960772 ],\n",
              "       [0.49617684],\n",
              "       [0.49616638],\n",
              "       [0.4960951 ],\n",
              "       [0.4962124 ],\n",
              "       [0.4960522 ],\n",
              "       [0.4960644 ],\n",
              "       [0.49599797],\n",
              "       [0.49618536],\n",
              "       [0.49610445],\n",
              "       [0.49615893],\n",
              "       [0.49614474],\n",
              "       [0.4960469 ],\n",
              "       [0.4960765 ],\n",
              "       [0.49613485],\n",
              "       [0.496173  ],\n",
              "       [0.4961249 ],\n",
              "       [0.49614552],\n",
              "       [0.49609444],\n",
              "       [0.4961939 ],\n",
              "       [0.49612936],\n",
              "       [0.49610835],\n",
              "       [0.49603838],\n",
              "       [0.49605447],\n",
              "       [0.4961439 ],\n",
              "       [0.49612102],\n",
              "       [0.49606764],\n",
              "       [0.4960703 ],\n",
              "       [0.49609858],\n",
              "       [0.49591988],\n",
              "       [0.49601692],\n",
              "       [0.4961171 ],\n",
              "       [0.49593046],\n",
              "       [0.49617642],\n",
              "       [0.49609956],\n",
              "       [0.4961587 ],\n",
              "       [0.4960313 ],\n",
              "       [0.49605078],\n",
              "       [0.4961431 ],\n",
              "       [0.4962153 ],\n",
              "       [0.4961479 ],\n",
              "       [0.4962056 ],\n",
              "       [0.49601936],\n",
              "       [0.49602264],\n",
              "       [0.4960999 ],\n",
              "       [0.49616778],\n",
              "       [0.49604857],\n",
              "       [0.49611485],\n",
              "       [0.49559683],\n",
              "       [0.49555767],\n",
              "       [0.49601457],\n",
              "       [0.4960861 ],\n",
              "       [0.4960696 ],\n",
              "       [0.49608514],\n",
              "       [0.49614096],\n",
              "       [0.49539044],\n",
              "       [0.4960832 ],\n",
              "       [0.49606368],\n",
              "       [0.49610084],\n",
              "       [0.4957505 ],\n",
              "       [0.4961002 ],\n",
              "       [0.49620008],\n",
              "       [0.49601424],\n",
              "       [0.4961131 ],\n",
              "       [0.49610916],\n",
              "       [0.49608785],\n",
              "       [0.496167  ],\n",
              "       [0.49606535],\n",
              "       [0.4961481 ],\n",
              "       [0.4959937 ],\n",
              "       [0.49604166],\n",
              "       [0.49598154],\n",
              "       [0.49601904],\n",
              "       [0.4961132 ],\n",
              "       [0.49617186],\n",
              "       [0.49617943],\n",
              "       [0.49604246],\n",
              "       [0.49612448],\n",
              "       [0.49598163],\n",
              "       [0.4961487 ],\n",
              "       [0.49609426],\n",
              "       [0.4961866 ],\n",
              "       [0.49581906],\n",
              "       [0.49614435],\n",
              "       [0.4960899 ],\n",
              "       [0.49607876],\n",
              "       [0.4961701 ],\n",
              "       [0.4960366 ],\n",
              "       [0.49606803],\n",
              "       [0.49595374],\n",
              "       [0.49615276],\n",
              "       [0.49598637],\n",
              "       [0.49615988],\n",
              "       [0.49604294],\n",
              "       [0.4960633 ],\n",
              "       [0.49603528],\n",
              "       [0.49613693],\n",
              "       [0.49604937],\n",
              "       [0.49608076],\n",
              "       [0.49567783],\n",
              "       [0.49615684],\n",
              "       [0.49598408],\n",
              "       [0.49584946],\n",
              "       [0.49618167],\n",
              "       [0.49614686],\n",
              "       [0.49606287],\n",
              "       [0.4961253 ],\n",
              "       [0.49610525],\n",
              "       [0.49585328],\n",
              "       [0.49605697],\n",
              "       [0.49575892],\n",
              "       [0.49620566],\n",
              "       [0.4960817 ],\n",
              "       [0.49599013],\n",
              "       [0.49610144],\n",
              "       [0.4961706 ],\n",
              "       [0.4960013 ],\n",
              "       [0.49610543],\n",
              "       [0.49611872],\n",
              "       [0.49614307],\n",
              "       [0.4959974 ],\n",
              "       [0.4958662 ],\n",
              "       [0.4961444 ],\n",
              "       [0.4961256 ],\n",
              "       [0.49566868],\n",
              "       [0.49544677],\n",
              "       [0.4961034 ],\n",
              "       [0.49614424],\n",
              "       [0.49593133],\n",
              "       [0.49601233],\n",
              "       [0.49614543],\n",
              "       [0.49621677],\n",
              "       [0.49614578],\n",
              "       [0.49605182],\n",
              "       [0.49614835],\n",
              "       [0.49615827],\n",
              "       [0.4959132 ],\n",
              "       [0.49602103],\n",
              "       [0.49608448],\n",
              "       [0.49615836],\n",
              "       [0.49605325],\n",
              "       [0.49617404],\n",
              "       [0.49620852],\n",
              "       [0.4960961 ],\n",
              "       [0.49613   ],\n",
              "       [0.4961728 ],\n",
              "       [0.4961211 ],\n",
              "       [0.49613175],\n",
              "       [0.4961604 ],\n",
              "       [0.49614066],\n",
              "       [0.49614623],\n",
              "       [0.49599704],\n",
              "       [0.4960455 ],\n",
              "       [0.49610466],\n",
              "       [0.49611637],\n",
              "       [0.49611294],\n",
              "       [0.49599376],\n",
              "       [0.49608365],\n",
              "       [0.49612302],\n",
              "       [0.49617016],\n",
              "       [0.49621913],\n",
              "       [0.4961458 ],\n",
              "       [0.4958637 ],\n",
              "       [0.49617648],\n",
              "       [0.49602646],\n",
              "       [0.4960068 ],\n",
              "       [0.49604943],\n",
              "       [0.49603304],\n",
              "       [0.49586454],\n",
              "       [0.4961478 ],\n",
              "       [0.49620146],\n",
              "       [0.49608818],\n",
              "       [0.49612   ],\n",
              "       [0.49614617],\n",
              "       [0.49591237],\n",
              "       [0.49592835],\n",
              "       [0.49604687],\n",
              "       [0.49616504],\n",
              "       [0.49610585],\n",
              "       [0.49604955],\n",
              "       [0.4960211 ],\n",
              "       [0.49615252],\n",
              "       [0.49608982],\n",
              "       [0.49611995],\n",
              "       [0.49609688],\n",
              "       [0.4959893 ],\n",
              "       [0.49595603],\n",
              "       [0.4960066 ],\n",
              "       [0.49615288],\n",
              "       [0.496037  ],\n",
              "       [0.4961243 ],\n",
              "       [0.49612868],\n",
              "       [0.49616975],\n",
              "       [0.49616152],\n",
              "       [0.49607173],\n",
              "       [0.49611184],\n",
              "       [0.4961288 ],\n",
              "       [0.49615106],\n",
              "       [0.49602392],\n",
              "       [0.49598178],\n",
              "       [0.49615547],\n",
              "       [0.495611  ],\n",
              "       [0.49615666],\n",
              "       [0.49619475],\n",
              "       [0.4960644 ],\n",
              "       [0.4961291 ],\n",
              "       [0.4961265 ],\n",
              "       [0.49606562],\n",
              "       [0.49614528],\n",
              "       [0.49614   ],\n",
              "       [0.49603504],\n",
              "       [0.49618667],\n",
              "       [0.49615696],\n",
              "       [0.49613777],\n",
              "       [0.495945  ],\n",
              "       [0.49614504],\n",
              "       [0.49600378],\n",
              "       [0.4960431 ],\n",
              "       [0.49608502],\n",
              "       [0.49616754],\n",
              "       [0.495918  ],\n",
              "       [0.49590042],\n",
              "       [0.4961843 ],\n",
              "       [0.49615157],\n",
              "       [0.4958742 ],\n",
              "       [0.49615395],\n",
              "       [0.4961697 ],\n",
              "       [0.4960723 ],\n",
              "       [0.49612865],\n",
              "       [0.49614576],\n",
              "       [0.49613363],\n",
              "       [0.49595034],\n",
              "       [0.4960633 ],\n",
              "       [0.49612844],\n",
              "       [0.49614042],\n",
              "       [0.49607113],\n",
              "       [0.4960025 ],\n",
              "       [0.49603337],\n",
              "       [0.49514765],\n",
              "       [0.49604663]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0IYWxDHSZ4v",
        "colab_type": "code",
        "outputId": "7cd82795-ec46-47b3-a8f4-72cf96347482",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "lstm_attention.layers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tensorflow.python.keras.engine.input_layer.InputLayer at 0x7fd7ee965a58>,\n",
              " <tensorflow.python.keras.layers.embeddings.Embedding at 0x7fd7ee965f28>,\n",
              " <tensorflow.python.keras.layers.recurrent.LSTM at 0x7fd7ee965828>,\n",
              " <tensorflow.python.keras.layers.core.Dense at 0x7fd7ee918cc0>,\n",
              " <tensorflow.python.keras.layers.core.Flatten at 0x7fd7ee93cf28>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7fd7ee8a8e10>,\n",
              " <tensorflow.python.keras.layers.core.RepeatVector at 0x7fd7ee918860>,\n",
              " <tensorflow.python.keras.layers.core.Permute at 0x7fd7ee84a7b8>,\n",
              " <tensorflow.python.keras.layers.merge.Multiply at 0x7fd7ee84ad68>,\n",
              " <tensorflow.python.keras.layers.core.Lambda at 0x7fd7ee8507f0>,\n",
              " <tensorflow.python.keras.layers.core.Dense at 0x7fd7ee850748>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLjq0aalSmrq",
        "colab_type": "code",
        "outputId": "87e5072b-3343-4969-cc60-08160016fe13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "for layer in lstm_attention.layers:\n",
        "    print(layer.output)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"input_16:0\", shape=(?, 1695), dtype=float32)\n",
            "Tensor(\"embedding_24/embedding_lookup/Identity_1:0\", shape=(?, 1695, 64), dtype=float32)\n",
            "Tensor(\"lstm_24/transpose_1:0\", shape=(?, 1695, 10), dtype=float32)\n",
            "Tensor(\"dense_25/Tanh:0\", shape=(?, 1695, 1), dtype=float32)\n",
            "Tensor(\"flatten_24/Reshape:0\", shape=(?, 1695), dtype=float32)\n",
            "Tensor(\"activation_12/Softmax:0\", shape=(?, 1695), dtype=float32)\n",
            "Tensor(\"repeat_vector_12/Tile:0\", shape=(?, 10, 1695), dtype=float32)\n",
            "Tensor(\"permute_12/transpose:0\", shape=(?, 1695, 10), dtype=float32)\n",
            "Tensor(\"multiply_12/mul:0\", shape=(?, 1695, 10), dtype=float32)\n",
            "Tensor(\"lambda_12/Sum:0\", shape=(?, 10), dtype=float32)\n",
            "Tensor(\"dense_26/Sigmoid:0\", shape=(?, 1), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQOZ1gFvtz9c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create Bidirectional LSTM with Attention using Keras Functional API\n",
        "def create_bi_lstm_with_attention():\n",
        "    _input = Input(shape=[MAX_LENGTH])\n",
        "    embedded = Embedding(\n",
        "            input_dim=NUM_CHARS,\n",
        "            output_dim=EMBEDDING_SIZE,\n",
        "            input_length=MAX_LENGTH,\n",
        "            trainable=False,\n",
        "            mask_zero=False\n",
        "        )(_input)\n",
        "    activations = Bidirectional(LSTM(HIDDEN_SIZE, return_sequences=True))(embedded)\n",
        "    # Attention\n",
        "    attention = Dense(1, activation='tanh')(activations)\n",
        "    attention = Flatten()(attention)\n",
        "    attention = Activation('softmax')(attention)\n",
        "    attention = RepeatVector(2*HIDDEN_SIZE)(attention)\n",
        "    attention = Permute([2, 1])(attention)\n",
        "    output_attention = multiply([activations, attention])\n",
        "\n",
        "    # output_attention = LSTM(HIDDEN_SIZE)(output_attention)\n",
        "    output_attention = Lambda(lambda xin: K.sum(xin, axis=-2), output_shape=(HIDDEN_SIZE,))(output_attention)\n",
        "    output = Dense(1, activation='sigmoid')(output_attention)\n",
        "    bi_lstm_attention = Model(inputs=[_input], outputs=output, name=\"bi_lstm_attention\")\n",
        "    return bi_lstm_attention"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fp1clmCI-Wac",
        "colab_type": "code",
        "outputId": "7d5cb7c6-b8f3-4987-d48a-9f8bcff9ece8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        }
      },
      "source": [
        "bi_lstm_attention = create_bi_lstm_with_attention()\n",
        "bi_lstm_attention.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(bi_lstm_attention.summary())\n",
        "\n",
        "bi_lstm_attention.fit(X_train_main, y_train_main,\n",
        "                         validation_data=(X_train_val, y_train_val),\n",
        "                         epochs=1, batch_size=1024, class_weight=class_weights_dict)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"bi_lstm_attention\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_19 (InputLayer)           [(None, 1695)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_27 (Embedding)        (None, 1695, 64)     8192        input_19[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_2 (Bidirectional) (None, 1695, 20)     6000        embedding_27[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_30 (Dense)                (None, 1695, 1)      21          bidirectional_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "flatten_28 (Flatten)            (None, 1695)         0           dense_30[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 1695)         0           flatten_28[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "repeat_vector_15 (RepeatVector) (None, 20, 1695)     0           activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "permute_15 (Permute)            (None, 1695, 20)     0           repeat_vector_15[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "multiply_15 (Multiply)          (None, 1695, 20)     0           bidirectional_2[0][0]            \n",
            "                                                                 permute_15[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_13 (Lambda)              (None, 20)           0           multiply_15[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_31 (Dense)                (None, 1)            21          lambda_13[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 14,234\n",
            "Trainable params: 6,042\n",
            "Non-trainable params: 8,192\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Train on 1683 samples, validate on 421 samples\n",
            "1683/1683 [==============================] - 12s 7ms/sample - loss: 0.6847 - acc: 0.9186 - val_loss: 0.6817 - val_acc: 0.9074\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7faf9f19fc88>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFpIZ1p27lrt",
        "colab_type": "code",
        "outputId": "4f42c159-c118-4118-bb92-60eb0a7ce4a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "for layer in bi_lstm_attention.layers:\n",
        "    print(layer.output)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"input_19:0\", shape=(?, 1695), dtype=float32)\n",
            "Tensor(\"embedding_27/embedding_lookup/Identity_1:0\", shape=(?, 1695, 64), dtype=float32)\n",
            "Tensor(\"bidirectional_2/concat:0\", shape=(?, 1695, 20), dtype=float32)\n",
            "Tensor(\"dense_30/Tanh:0\", shape=(?, 1695, 1), dtype=float32)\n",
            "Tensor(\"flatten_28/Reshape:0\", shape=(?, 1695), dtype=float32)\n",
            "Tensor(\"activation_15/Softmax:0\", shape=(?, 1695), dtype=float32)\n",
            "Tensor(\"repeat_vector_15/Tile:0\", shape=(?, 20, 1695), dtype=float32)\n",
            "Tensor(\"permute_15/transpose:0\", shape=(?, 1695, 20), dtype=float32)\n",
            "Tensor(\"multiply_15/mul:0\", shape=(?, 1695, 20), dtype=float32)\n",
            "Tensor(\"lambda_13/Sum:0\", shape=(?, 20), dtype=float32)\n",
            "Tensor(\"dense_31/Sigmoid:0\", shape=(?, 1), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbNYZ3mk-bpB",
        "colab_type": "code",
        "outputId": "df6d07af-2008-4fe4-e946-1cd02ba63f79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "predictions_bi_lstm_attention, rounded_bi_lstm_attention = evaluate_metrics(bi_lstm_attention, X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision: 0.920303605313093\n",
            "Recall: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6v5LOAh8HRD",
        "colab_type": "code",
        "outputId": "e6d5a277-d507-4a79-dc5c-a07318f61140",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "predictions_bi_lstm_attention"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.49279505],\n",
              "       [0.49278334],\n",
              "       [0.49282378],\n",
              "       [0.49290022],\n",
              "       [0.49274424],\n",
              "       [0.49280027],\n",
              "       [0.49284464],\n",
              "       [0.49289772],\n",
              "       [0.49282852],\n",
              "       [0.49298853],\n",
              "       [0.49282432],\n",
              "       [0.4929398 ],\n",
              "       [0.49289438],\n",
              "       [0.49284768],\n",
              "       [0.4928056 ],\n",
              "       [0.49273527],\n",
              "       [0.4928078 ],\n",
              "       [0.49285936],\n",
              "       [0.49352923],\n",
              "       [0.49284142],\n",
              "       [0.4928773 ],\n",
              "       [0.49280974],\n",
              "       [0.49281394],\n",
              "       [0.49290928],\n",
              "       [0.4928471 ],\n",
              "       [0.4927938 ],\n",
              "       [0.49299645],\n",
              "       [0.49297532],\n",
              "       [0.49281278],\n",
              "       [0.4928103 ],\n",
              "       [0.49279988],\n",
              "       [0.49309644],\n",
              "       [0.49296218],\n",
              "       [0.49295136],\n",
              "       [0.4928891 ],\n",
              "       [0.49289304],\n",
              "       [0.4927944 ],\n",
              "       [0.49284866],\n",
              "       [0.49278468],\n",
              "       [0.4928492 ],\n",
              "       [0.49282867],\n",
              "       [0.4928429 ],\n",
              "       [0.49317837],\n",
              "       [0.49278235],\n",
              "       [0.4929482 ],\n",
              "       [0.49283925],\n",
              "       [0.4927866 ],\n",
              "       [0.49281675],\n",
              "       [0.4931284 ],\n",
              "       [0.49293962],\n",
              "       [0.49313477],\n",
              "       [0.49281687],\n",
              "       [0.49280658],\n",
              "       [0.49279994],\n",
              "       [0.49326724],\n",
              "       [0.49280038],\n",
              "       [0.4929061 ],\n",
              "       [0.4928839 ],\n",
              "       [0.49301228],\n",
              "       [0.49281955],\n",
              "       [0.49290466],\n",
              "       [0.4928171 ],\n",
              "       [0.49288636],\n",
              "       [0.49277282],\n",
              "       [0.4927874 ],\n",
              "       [0.4928192 ],\n",
              "       [0.49301073],\n",
              "       [0.49285075],\n",
              "       [0.49286795],\n",
              "       [0.49283007],\n",
              "       [0.49281028],\n",
              "       [0.49285182],\n",
              "       [0.4929778 ],\n",
              "       [0.49296543],\n",
              "       [0.49280322],\n",
              "       [0.49294522],\n",
              "       [0.4928497 ],\n",
              "       [0.4929689 ],\n",
              "       [0.4928469 ],\n",
              "       [0.49285722],\n",
              "       [0.49282432],\n",
              "       [0.4928398 ],\n",
              "       [0.49280164],\n",
              "       [0.49279293],\n",
              "       [0.49281064],\n",
              "       [0.49281633],\n",
              "       [0.4928237 ],\n",
              "       [0.49291754],\n",
              "       [0.4928375 ],\n",
              "       [0.49292257],\n",
              "       [0.49295202],\n",
              "       [0.4928154 ],\n",
              "       [0.4929197 ],\n",
              "       [0.4927884 ],\n",
              "       [0.4928376 ],\n",
              "       [0.4929834 ],\n",
              "       [0.4929631 ],\n",
              "       [0.4928303 ],\n",
              "       [0.4928198 ],\n",
              "       [0.49282423],\n",
              "       [0.49295586],\n",
              "       [0.49288374],\n",
              "       [0.49279946],\n",
              "       [0.492808  ],\n",
              "       [0.4927904 ],\n",
              "       [0.4928162 ],\n",
              "       [0.49282998],\n",
              "       [0.49277833],\n",
              "       [0.49273598],\n",
              "       [0.49290815],\n",
              "       [0.4928766 ],\n",
              "       [0.49283323],\n",
              "       [0.49283266],\n",
              "       [0.4928056 ],\n",
              "       [0.4928623 ],\n",
              "       [0.49285668],\n",
              "       [0.49285072],\n",
              "       [0.4930101 ],\n",
              "       [0.4928513 ],\n",
              "       [0.4927707 ],\n",
              "       [0.49285114],\n",
              "       [0.49300918],\n",
              "       [0.49285772],\n",
              "       [0.49286094],\n",
              "       [0.49288806],\n",
              "       [0.49287325],\n",
              "       [0.4928072 ],\n",
              "       [0.4928611 ],\n",
              "       [0.49282196],\n",
              "       [0.49291086],\n",
              "       [0.4928911 ],\n",
              "       [0.49282962],\n",
              "       [0.49287167],\n",
              "       [0.4929827 ],\n",
              "       [0.49283785],\n",
              "       [0.49296495],\n",
              "       [0.49279508],\n",
              "       [0.49287355],\n",
              "       [0.49298352],\n",
              "       [0.49291933],\n",
              "       [0.49289474],\n",
              "       [0.4928863 ],\n",
              "       [0.4928736 ],\n",
              "       [0.49280235],\n",
              "       [0.49281108],\n",
              "       [0.49281904],\n",
              "       [0.49283582],\n",
              "       [0.49282953],\n",
              "       [0.4928184 ],\n",
              "       [0.49285954],\n",
              "       [0.4927393 ],\n",
              "       [0.49293298],\n",
              "       [0.49284762],\n",
              "       [0.4930486 ],\n",
              "       [0.49282867],\n",
              "       [0.49275804],\n",
              "       [0.49277705],\n",
              "       [0.49289012],\n",
              "       [0.49280903],\n",
              "       [0.4928172 ],\n",
              "       [0.49279258],\n",
              "       [0.49287018],\n",
              "       [0.4928567 ],\n",
              "       [0.4930278 ],\n",
              "       [0.49285844],\n",
              "       [0.49284002],\n",
              "       [0.4929192 ],\n",
              "       [0.49278447],\n",
              "       [0.4929124 ],\n",
              "       [0.49287707],\n",
              "       [0.49301118],\n",
              "       [0.49286848],\n",
              "       [0.49296868],\n",
              "       [0.49289086],\n",
              "       [0.49279872],\n",
              "       [0.4929083 ],\n",
              "       [0.49287745],\n",
              "       [0.49294877],\n",
              "       [0.49280018],\n",
              "       [0.49286577],\n",
              "       [0.4928836 ],\n",
              "       [0.49291274],\n",
              "       [0.4928261 ],\n",
              "       [0.49278647],\n",
              "       [0.4927713 ],\n",
              "       [0.49285686],\n",
              "       [0.49297532],\n",
              "       [0.49302086],\n",
              "       [0.4929124 ],\n",
              "       [0.4929829 ],\n",
              "       [0.4928729 ],\n",
              "       [0.49287683],\n",
              "       [0.49291047],\n",
              "       [0.4928295 ],\n",
              "       [0.4928113 ],\n",
              "       [0.49277195],\n",
              "       [0.4929566 ],\n",
              "       [0.49280804],\n",
              "       [0.49287203],\n",
              "       [0.4928364 ],\n",
              "       [0.49280694],\n",
              "       [0.49309802],\n",
              "       [0.492889  ],\n",
              "       [0.49300832],\n",
              "       [0.49313098],\n",
              "       [0.49299192],\n",
              "       [0.49288777],\n",
              "       [0.4928191 ],\n",
              "       [0.4929614 ],\n",
              "       [0.49294594],\n",
              "       [0.49285147],\n",
              "       [0.49299654],\n",
              "       [0.49280083],\n",
              "       [0.4927949 ],\n",
              "       [0.49300474],\n",
              "       [0.49278995],\n",
              "       [0.49290878],\n",
              "       [0.49287644],\n",
              "       [0.49283093],\n",
              "       [0.49301022],\n",
              "       [0.49289724],\n",
              "       [0.49284244],\n",
              "       [0.49280405],\n",
              "       [0.49283817],\n",
              "       [0.4929718 ],\n",
              "       [0.49281928],\n",
              "       [0.49287993],\n",
              "       [0.4928078 ],\n",
              "       [0.49281073],\n",
              "       [0.49297124],\n",
              "       [0.49287948],\n",
              "       [0.49282807],\n",
              "       [0.4928018 ],\n",
              "       [0.49278012],\n",
              "       [0.4928253 ],\n",
              "       [0.49284765],\n",
              "       [0.49311697],\n",
              "       [0.49297887],\n",
              "       [0.49282393],\n",
              "       [0.49332714],\n",
              "       [0.49281293],\n",
              "       [0.49317548],\n",
              "       [0.49280474],\n",
              "       [0.49297163],\n",
              "       [0.49295786],\n",
              "       [0.492886  ],\n",
              "       [0.49290037],\n",
              "       [0.4928609 ],\n",
              "       [0.49295664],\n",
              "       [0.49279076],\n",
              "       [0.4928484 ],\n",
              "       [0.4929145 ],\n",
              "       [0.4929569 ],\n",
              "       [0.49313086],\n",
              "       [0.49300957],\n",
              "       [0.49292845],\n",
              "       [0.49287978],\n",
              "       [0.49278384],\n",
              "       [0.49295634],\n",
              "       [0.49280393],\n",
              "       [0.49288365],\n",
              "       [0.49285248],\n",
              "       [0.49280262],\n",
              "       [0.49284637],\n",
              "       [0.4927918 ],\n",
              "       [0.49281663],\n",
              "       [0.49281543],\n",
              "       [0.4928956 ],\n",
              "       [0.49285725],\n",
              "       [0.4928594 ],\n",
              "       [0.49292952],\n",
              "       [0.49283963],\n",
              "       [0.493017  ],\n",
              "       [0.49286875],\n",
              "       [0.49280986],\n",
              "       [0.49279448],\n",
              "       [0.49297926],\n",
              "       [0.49299344],\n",
              "       [0.49278942],\n",
              "       [0.4928251 ],\n",
              "       [0.4928455 ],\n",
              "       [0.4928611 ],\n",
              "       [0.4928017 ],\n",
              "       [0.49283472],\n",
              "       [0.4929812 ],\n",
              "       [0.4928216 ],\n",
              "       [0.4927634 ],\n",
              "       [0.49284896],\n",
              "       [0.4928562 ],\n",
              "       [0.49286968],\n",
              "       [0.49291986],\n",
              "       [0.492813  ],\n",
              "       [0.49295595],\n",
              "       [0.4928459 ],\n",
              "       [0.49289644],\n",
              "       [0.49279508],\n",
              "       [0.4929322 ],\n",
              "       [0.49278998],\n",
              "       [0.49280342],\n",
              "       [0.4928881 ],\n",
              "       [0.49280357],\n",
              "       [0.49284545],\n",
              "       [0.4927986 ],\n",
              "       [0.49287504],\n",
              "       [0.49279344],\n",
              "       [0.4927499 ],\n",
              "       [0.4928492 ],\n",
              "       [0.4928191 ],\n",
              "       [0.4928521 ],\n",
              "       [0.49285686],\n",
              "       [0.4929488 ],\n",
              "       [0.49286482],\n",
              "       [0.49284744],\n",
              "       [0.49286208],\n",
              "       [0.49280313],\n",
              "       [0.4928463 ],\n",
              "       [0.49293458],\n",
              "       [0.49297392],\n",
              "       [0.4928374 ],\n",
              "       [0.49293151],\n",
              "       [0.49278712],\n",
              "       [0.49279892],\n",
              "       [0.49283424],\n",
              "       [0.49295914],\n",
              "       [0.49278918],\n",
              "       [0.49285448],\n",
              "       [0.49277368],\n",
              "       [0.49278393],\n",
              "       [0.49281567],\n",
              "       [0.4928798 ],\n",
              "       [0.49300513],\n",
              "       [0.49284816],\n",
              "       [0.49284273],\n",
              "       [0.492949  ],\n",
              "       [0.4928896 ],\n",
              "       [0.4932749 ],\n",
              "       [0.49307063],\n",
              "       [0.4926902 ],\n",
              "       [0.4928054 ],\n",
              "       [0.49291465],\n",
              "       [0.49286816],\n",
              "       [0.49285313],\n",
              "       [0.49320874],\n",
              "       [0.49286422],\n",
              "       [0.49294135],\n",
              "       [0.4928185 ],\n",
              "       [0.4930323 ],\n",
              "       [0.49289185],\n",
              "       [0.4928425 ],\n",
              "       [0.49292508],\n",
              "       [0.49293318],\n",
              "       [0.4928133 ],\n",
              "       [0.49296376],\n",
              "       [0.49280038],\n",
              "       [0.49288562],\n",
              "       [0.4928182 ],\n",
              "       [0.49296707],\n",
              "       [0.49292234],\n",
              "       [0.49305207],\n",
              "       [0.49285665],\n",
              "       [0.49281982],\n",
              "       [0.49283522],\n",
              "       [0.4928    ],\n",
              "       [0.49290308],\n",
              "       [0.49286026],\n",
              "       [0.4929976 ],\n",
              "       [0.49282396],\n",
              "       [0.49285093],\n",
              "       [0.49284017],\n",
              "       [0.4930166 ],\n",
              "       [0.49290583],\n",
              "       [0.49281898],\n",
              "       [0.49282455],\n",
              "       [0.49279612],\n",
              "       [0.4928895 ],\n",
              "       [0.49279153],\n",
              "       [0.49296832],\n",
              "       [0.49277705],\n",
              "       [0.49297214],\n",
              "       [0.49281952],\n",
              "       [0.49297145],\n",
              "       [0.4929307 ],\n",
              "       [0.49295875],\n",
              "       [0.49285638],\n",
              "       [0.4928245 ],\n",
              "       [0.49292314],\n",
              "       [0.49309662],\n",
              "       [0.49277553],\n",
              "       [0.49304283],\n",
              "       [0.49310714],\n",
              "       [0.49284673],\n",
              "       [0.49286604],\n",
              "       [0.49293473],\n",
              "       [0.49275607],\n",
              "       [0.49281654],\n",
              "       [0.4930508 ],\n",
              "       [0.4928918 ],\n",
              "       [0.49294353],\n",
              "       [0.4928342 ],\n",
              "       [0.4928253 ],\n",
              "       [0.4929265 ],\n",
              "       [0.49277744],\n",
              "       [0.4928283 ],\n",
              "       [0.49293867],\n",
              "       [0.4928651 ],\n",
              "       [0.49282393],\n",
              "       [0.49283746],\n",
              "       [0.4928021 ],\n",
              "       [0.49307126],\n",
              "       [0.49283522],\n",
              "       [0.49280143],\n",
              "       [0.4930557 ],\n",
              "       [0.49329686],\n",
              "       [0.49286026],\n",
              "       [0.49281   ],\n",
              "       [0.4930007 ],\n",
              "       [0.4929498 ],\n",
              "       [0.49284074],\n",
              "       [0.49284333],\n",
              "       [0.49283034],\n",
              "       [0.4928875 ],\n",
              "       [0.4928043 ],\n",
              "       [0.49278376],\n",
              "       [0.4930641 ],\n",
              "       [0.49298796],\n",
              "       [0.49283433],\n",
              "       [0.49285185],\n",
              "       [0.49295798],\n",
              "       [0.49281377],\n",
              "       [0.49282986],\n",
              "       [0.49290955],\n",
              "       [0.49283695],\n",
              "       [0.4928393 ],\n",
              "       [0.49286085],\n",
              "       [0.49283725],\n",
              "       [0.49279696],\n",
              "       [0.49283585],\n",
              "       [0.49279392],\n",
              "       [0.49299008],\n",
              "       [0.49295425],\n",
              "       [0.49284247],\n",
              "       [0.49281067],\n",
              "       [0.49288043],\n",
              "       [0.49289563],\n",
              "       [0.49286556],\n",
              "       [0.49279025],\n",
              "       [0.49279636],\n",
              "       [0.4927874 ],\n",
              "       [0.49279955],\n",
              "       [0.49318105],\n",
              "       [0.49283332],\n",
              "       [0.49293342],\n",
              "       [0.49297124],\n",
              "       [0.49285728],\n",
              "       [0.49296257],\n",
              "       [0.49293563],\n",
              "       [0.4927437 ],\n",
              "       [0.49280143],\n",
              "       [0.4928598 ],\n",
              "       [0.49281245],\n",
              "       [0.4927851 ],\n",
              "       [0.49291307],\n",
              "       [0.49290887],\n",
              "       [0.49292156],\n",
              "       [0.4928108 ],\n",
              "       [0.49278197],\n",
              "       [0.4929161 ],\n",
              "       [0.49296647],\n",
              "       [0.49297515],\n",
              "       [0.49285433],\n",
              "       [0.49285817],\n",
              "       [0.49280626],\n",
              "       [0.49297774],\n",
              "       [0.49281067],\n",
              "       [0.49292624],\n",
              "       [0.49284023],\n",
              "       [0.49302882],\n",
              "       [0.49279577],\n",
              "       [0.49281943],\n",
              "       [0.49279645],\n",
              "       [0.4927979 ],\n",
              "       [0.49291703],\n",
              "       [0.4928227 ],\n",
              "       [0.49284178],\n",
              "       [0.49281996],\n",
              "       [0.49298516],\n",
              "       [0.49296007],\n",
              "       [0.49288625],\n",
              "       [0.4929427 ],\n",
              "       [0.49277353],\n",
              "       [0.49284104],\n",
              "       [0.49283782],\n",
              "       [0.49281842],\n",
              "       [0.49284518],\n",
              "       [0.49287197],\n",
              "       [0.49272794],\n",
              "       [0.49280384],\n",
              "       [0.4928883 ],\n",
              "       [0.49280477],\n",
              "       [0.49283564],\n",
              "       [0.49285167],\n",
              "       [0.49292436],\n",
              "       [0.49275985],\n",
              "       [0.49303192],\n",
              "       [0.49296254],\n",
              "       [0.49279237],\n",
              "       [0.4927848 ],\n",
              "       [0.49293956],\n",
              "       [0.49306643],\n",
              "       [0.49281475],\n",
              "       [0.4927333 ],\n",
              "       [0.49306446],\n",
              "       [0.4928183 ],\n",
              "       [0.49282345],\n",
              "       [0.4928454 ],\n",
              "       [0.49287516],\n",
              "       [0.49281195],\n",
              "       [0.49283573],\n",
              "       [0.49297893],\n",
              "       [0.49296007],\n",
              "       [0.4928437 ],\n",
              "       [0.4928796 ],\n",
              "       [0.4928818 ],\n",
              "       [0.4929433 ],\n",
              "       [0.4928622 ],\n",
              "       [0.49333245],\n",
              "       [0.49293765]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FTdqroT8I3a",
        "colab_type": "code",
        "outputId": "c9630e4b-f64d-4b6a-ed0b-e09feafdadf7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# MLP\n",
        "MLP_HIDDEN_SIZE = 64\n",
        "\n",
        "mlp = Sequential()\n",
        "mlp.add(Input(shape=(MAX_LENGTH,)))\n",
        "mlp.add(Dense(MLP_HIDDEN_SIZE, activation='relu'))\n",
        "mlp.add(Dropout(0.5))\n",
        "mlp.add(Dense(MLP_HIDDEN_SIZE, activation='relu'))  # 'tanh'\n",
        "mlp.add(Dense(MLP_HIDDEN_SIZE, activation='relu'))  # 'tanh'\n",
        "\n",
        "mlp.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# adam = optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "mlp.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(mlp.summary())\n",
        "\n",
        "mlp.fit(X_train, y_train,\n",
        "                        #  validation_data=(X_train_val, y_train_val),\n",
        "                         epochs=50, batch_size=64, class_weight=class_weights_dict)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_9 (Dense)              (None, 64)                108544    \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 116,929\n",
            "Trainable params: 116,929\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 2104 samples\n",
            "Epoch 1/50\n",
            "2104/2104 [==============================] - 0s 116us/sample - loss: 1.9719 - acc: 0.5566\n",
            "Epoch 2/50\n",
            "2104/2104 [==============================] - 0s 57us/sample - loss: 1.0164 - acc: 0.4886\n",
            "Epoch 3/50\n",
            "2104/2104 [==============================] - 0s 52us/sample - loss: 0.8592 - acc: 0.4672\n",
            "Epoch 4/50\n",
            "2104/2104 [==============================] - 0s 58us/sample - loss: 0.7350 - acc: 0.5656\n",
            "Epoch 5/50\n",
            "2104/2104 [==============================] - 0s 56us/sample - loss: 0.8294 - acc: 0.4287\n",
            "Epoch 6/50\n",
            "2104/2104 [==============================] - 0s 55us/sample - loss: 0.7059 - acc: 0.4924\n",
            "Epoch 7/50\n",
            "2104/2104 [==============================] - 0s 55us/sample - loss: 0.6664 - acc: 0.4306\n",
            "Epoch 8/50\n",
            "2104/2104 [==============================] - 0s 58us/sample - loss: 0.7395 - acc: 0.4525\n",
            "Epoch 9/50\n",
            "2104/2104 [==============================] - 0s 61us/sample - loss: 0.6933 - acc: 0.3498\n",
            "Epoch 10/50\n",
            "2104/2104 [==============================] - 0s 56us/sample - loss: 0.6812 - acc: 0.3883\n",
            "Epoch 11/50\n",
            "2104/2104 [==============================] - 0s 58us/sample - loss: 0.6777 - acc: 0.3864\n",
            "Epoch 12/50\n",
            "2104/2104 [==============================] - 0s 63us/sample - loss: 0.7094 - acc: 0.4639\n",
            "Epoch 13/50\n",
            "2104/2104 [==============================] - 0s 61us/sample - loss: 0.6612 - acc: 0.4330\n",
            "Epoch 14/50\n",
            "2104/2104 [==============================] - 0s 58us/sample - loss: 0.6724 - acc: 0.3698\n",
            "Epoch 15/50\n",
            "2104/2104 [==============================] - 0s 59us/sample - loss: 0.6971 - acc: 0.3788\n",
            "Epoch 16/50\n",
            "2104/2104 [==============================] - 0s 62us/sample - loss: 0.6727 - acc: 0.4458\n",
            "Epoch 17/50\n",
            "2104/2104 [==============================] - 0s 55us/sample - loss: 0.7122 - acc: 0.3864\n",
            "Epoch 18/50\n",
            "2104/2104 [==============================] - 0s 55us/sample - loss: 0.6415 - acc: 0.4011\n",
            "Epoch 19/50\n",
            "2104/2104 [==============================] - 0s 55us/sample - loss: 0.6616 - acc: 0.4544\n",
            "Epoch 20/50\n",
            "2104/2104 [==============================] - 0s 59us/sample - loss: 0.6488 - acc: 0.3470\n",
            "Epoch 21/50\n",
            "2104/2104 [==============================] - 0s 62us/sample - loss: 0.6733 - acc: 0.4016\n",
            "Epoch 22/50\n",
            "2104/2104 [==============================] - 0s 56us/sample - loss: 0.6404 - acc: 0.3907\n",
            "Epoch 23/50\n",
            "2104/2104 [==============================] - 0s 60us/sample - loss: 0.6445 - acc: 0.3702\n",
            "Epoch 24/50\n",
            "2104/2104 [==============================] - 0s 59us/sample - loss: 0.6451 - acc: 0.3731\n",
            "Epoch 25/50\n",
            "2104/2104 [==============================] - 0s 61us/sample - loss: 0.7235 - acc: 0.2495\n",
            "Epoch 26/50\n",
            "2104/2104 [==============================] - 0s 60us/sample - loss: 0.6578 - acc: 0.3061\n",
            "Epoch 27/50\n",
            "2104/2104 [==============================] - 0s 61us/sample - loss: 0.6432 - acc: 0.3541\n",
            "Epoch 28/50\n",
            "2104/2104 [==============================] - 0s 55us/sample - loss: 0.6653 - acc: 0.3047\n",
            "Epoch 29/50\n",
            "2104/2104 [==============================] - 0s 57us/sample - loss: 0.6431 - acc: 0.3617\n",
            "Epoch 30/50\n",
            "2104/2104 [==============================] - 0s 55us/sample - loss: 0.6455 - acc: 0.3146\n",
            "Epoch 31/50\n",
            "2104/2104 [==============================] - 0s 59us/sample - loss: 0.6582 - acc: 0.2776\n",
            "Epoch 32/50\n",
            "2104/2104 [==============================] - 0s 61us/sample - loss: 0.6481 - acc: 0.2875\n",
            "Epoch 33/50\n",
            "2104/2104 [==============================] - 0s 56us/sample - loss: 0.6549 - acc: 0.3683\n",
            "Epoch 34/50\n",
            "2104/2104 [==============================] - 0s 56us/sample - loss: 0.6507 - acc: 0.2676\n",
            "Epoch 35/50\n",
            "2104/2104 [==============================] - 0s 57us/sample - loss: 0.6511 - acc: 0.2975\n",
            "Epoch 36/50\n",
            "2104/2104 [==============================] - 0s 57us/sample - loss: 0.6441 - acc: 0.3265\n",
            "Epoch 37/50\n",
            "2104/2104 [==============================] - 0s 59us/sample - loss: 0.6444 - acc: 0.3522\n",
            "Epoch 38/50\n",
            "2104/2104 [==============================] - 0s 57us/sample - loss: 0.6478 - acc: 0.3550\n",
            "Epoch 39/50\n",
            "2104/2104 [==============================] - 0s 59us/sample - loss: 0.6598 - acc: 0.3593\n",
            "Epoch 40/50\n",
            "2104/2104 [==============================] - 0s 60us/sample - loss: 0.6417 - acc: 0.2847\n",
            "Epoch 41/50\n",
            "2104/2104 [==============================] - 0s 57us/sample - loss: 0.6436 - acc: 0.2904\n",
            "Epoch 42/50\n",
            "2104/2104 [==============================] - 0s 57us/sample - loss: 0.6581 - acc: 0.2324\n",
            "Epoch 43/50\n",
            "2104/2104 [==============================] - 0s 58us/sample - loss: 0.6462 - acc: 0.2490\n",
            "Epoch 44/50\n",
            "2104/2104 [==============================] - 0s 60us/sample - loss: 0.6426 - acc: 0.2429\n",
            "Epoch 45/50\n",
            "2104/2104 [==============================] - 0s 58us/sample - loss: 0.6384 - acc: 0.2514\n",
            "Epoch 46/50\n",
            "2104/2104 [==============================] - 0s 58us/sample - loss: 0.6510 - acc: 0.2576\n",
            "Epoch 47/50\n",
            "2104/2104 [==============================] - 0s 55us/sample - loss: 0.6318 - acc: 0.2761\n",
            "Epoch 48/50\n",
            "2104/2104 [==============================] - 0s 57us/sample - loss: 0.6372 - acc: 0.3218\n",
            "Epoch 49/50\n",
            "2104/2104 [==============================] - 0s 57us/sample - loss: 0.6290 - acc: 0.3508\n",
            "Epoch 50/50\n",
            "2104/2104 [==============================] - 0s 56us/sample - loss: 0.6251 - acc: 0.3736\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fa565500940>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2rkLZnrubD0",
        "colab_type": "code",
        "outputId": "d6b8611f-7b4c-4b23-f73f-412751e4efdf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "predictions_mlp, rounded_mlp = evaluate_metrics(mlp, X_test, y_test)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision: 0.963963963963964\n",
            "Recall: 0.44123711340206184\n",
            "Balanced Accuracy: 0.6253804614629357\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "603ZA6ioukkh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}